{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "from numpy.random import seed\n",
    "import joblib\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "import statistics\n",
    "\n",
    "#Reproducability of Results\n",
    "seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,number_of_inputs,hidden_layer_size,number_of_outputs):\n",
    "        super(DNNRegression, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_layer_size,128)\n",
    "        self.hidden3 = torch.nn.Linear(128,256)\n",
    "        self.hidden4 = torch.nn.Linear(256,512)\n",
    "        self.predict = torch.nn.Linear(512,number_of_outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.predict(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Import and Transform into Torch Training/Testing Tensors\n",
    "label_enc_tx = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "label_enc_no_tx = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "def Dataset(with_tx):\n",
    "    if with_tx:\n",
    "        data = pd.read_csv('Dataset/data_tx.csv')\n",
    "        train_data, test_data = train_test_split(data.values,test_size=0.2,shuffle=True)\n",
    "        return label_enc_tx.fit_transform(train_data),label_enc_tx.transform(test_data)\n",
    "    else:\n",
    "        data = pd.read_csv('Dataset/data_no_tx.csv')\n",
    "        train_data,test_data = train_test_split(data.values,test_size=0.2,shuffle=True)\n",
    "    return label_enc_no_tx.fit_transform(train_data),label_enc_no_tx.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549, 13) (7789, 10)\n"
     ]
    }
   ],
   "source": [
    "data_tx,_ = Dataset(True)\n",
    "data_no_tx, _ = Dataset(False)\n",
    "\n",
    "print(data_tx.shape,data_no_tx.shape)\n",
    "\n",
    "def load_predictor(with_tx):\n",
    "    if with_tx:\n",
    "        scaler = joblib.load('Models/label_enc_tx.save')\n",
    "        model = DNNRegression(data_tx.shape[1] - 3,64,3)\n",
    "        model.load_state_dict(torch.load('Models/model_tx.pt'))\n",
    "        return model,scaler\n",
    "    else:\n",
    "        scaler = joblib.load('Models/label_enc_no_tx.save')\n",
    "        model = DNNRegression(data_no_tx.shape[1] - 2,64,2)\n",
    "        model.load_state_dict(torch.load('Models/model_no_tx.pt'))\n",
    "        return model,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_GA(nn.Module):\n",
    "    #defines a Genetic Algorithm Approach to Reinforcement learning model\n",
    "    \n",
    "    def __init__(self,number_of_inputs,hidden_layer_size,number_of_outputs):\n",
    "        super(RL_GA,self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(number_of_inputs,hidden_layer_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_layer_size,number_of_outputs))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549, 13) (7789, 10)\n"
     ]
    }
   ],
   "source": [
    "#Import DNN Regression Model for EC/MS/TP predictions\n",
    "\n",
    "data_tx,_ = Dataset(True)\n",
    "data_no_tx, _ = Dataset(False)\n",
    "\n",
    "print(data_tx.shape,data_no_tx.shape)\n",
    "\n",
    "def load_predictor(with_tx):\n",
    "    if with_tx:\n",
    "        scaler = joblib.load('Models/label_enc_tx.save')\n",
    "        model = DNNRegression(data_tx.shape[1] - 3,64,3)\n",
    "        model.load_state_dict(torch.load('Models/model_tx.pt'))\n",
    "        return model,scaler\n",
    "    else:\n",
    "        scaler = joblib.load('Models/label_enc_no_tx.save')\n",
    "        model = DNNRegression(data_no_tx.shape[1] - 2,64,2)\n",
    "        model.load_state_dict(torch.load('Models/model_no_tx.pt'))\n",
    "        return model,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,with_tx,target_kpi):\n",
    "    model_state = {\n",
    "        'state_dict': model.state_dict()}\n",
    "    if with_tx: \n",
    "        tx = \"tx\"\n",
    "    else:\n",
    "        tx = \"no_tx\"\n",
    "    \n",
    "    model_save_name = F\"model_{tx}_GA_{target_kpi}.tar\"\n",
    "    path = F\"Models/{model_save_name}\"\n",
    "    torch.save(model_state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Action(output,epsilon):\n",
    "    action = torch.zeros([NUMBER_OF_OUTPUTS], dtype=torch.float32)\n",
    "    random_action = random.random() < 0\n",
    "\n",
    "    if random_action:\n",
    "        action[random.randrange(NUMBER_OF_OUTPUTS)] = 1\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            action[torch.argmax(output).item()] = 1\n",
    "            \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward Calculation\n",
    "\n",
    "def CalculateReward(current_state,epoch,target_kpi,with_tx,terminal):\n",
    "    if epoch == NUMBER_OF_SIM or terminal:\n",
    "        terminal = True \n",
    "    else: terminal = False\n",
    "    if with_tx:\n",
    "        ms = current_state[1]\n",
    "        ec = current_state[0]\n",
    "        tp = current_state[12]\n",
    "    else:\n",
    "        ms = current_state[1]\n",
    "        ec = current_state[0]\n",
    "    if target_kpi == \"ms\":\n",
    "        # goal is to reduce the ms\n",
    "        return 1-ms,terminal\n",
    "    elif target_kpi == \"ec\":\n",
    "        return 1-ec,terminal\n",
    "    elif target_kpi == \"tp\":\n",
    "        return tp,terminal\n",
    "    else:\n",
    "        if with_tx:\n",
    "            return ((1-ec) +tp + (1-ms))/3,terminal\n",
    "        else:\n",
    "            return 1-(0.5*ec+0.5*ms),terminal\n",
    "    return -1,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0, 100010, 101]\n",
    "scaled_timers_tx = [0.0,0.00100989901009899,0.9999999999999999]\n",
    "# [0, 1, 101, 100010, 11]\n",
    "scaled_timers_no_tx = [0.0,9.99900009999e-06,0.00010998900109989,0.00100989901009899,0.9999999999999999]\n",
    "packet_size_no_tx = [0.0,0.016,0.232,0.488,1.0]\n",
    "packet_size_tx = [ 0.0,0.21951219512195125,0.47967479674796754,1.0]\n",
    "\n",
    "def Increase_Timer(prev_timer,with_tx):\n",
    "    if with_tx:\n",
    "        index = scaled_timers_tx.index(prev_timer)\n",
    "        return scaled_timers_tx[min(index+1,2)],index==2\n",
    "    else:\n",
    "        index = scaled_timers_no_tx.index(prev_timer)\n",
    "        return scaled_timers_no_tx[min(index+1,2)],index==2\n",
    "\n",
    "def Decrease_Timer(prev_timer,with_tx):\n",
    "     if with_tx:\n",
    "        index = scaled_timers_tx.index(prev_timer)\n",
    "        return scaled_timers_tx[max(index-1,0)],index==0\n",
    "     else:\n",
    "        index = scaled_timers_no_tx.index(prev_timer)\n",
    "        return scaled_timers_no_tx[max(index-1,0)],index==0\n",
    "\n",
    "def Increase_Packet_size(prev_packet,with_tx):\n",
    "    if with_tx:\n",
    "        index = packet_size_tx.index(prev_packet)\n",
    "        return packet_size_tx[min(index+1,3)],index==3\n",
    "    else:\n",
    "        index = packet_size_no_tx.index(prev_packet)\n",
    "        return packet_size_no_tx[min(index+1,3)],index==3\n",
    "\n",
    "def Decrease_Packet_size(prev_packet,with_tx):\n",
    "    if with_tx:\n",
    "        index = packet_size_tx.index(prev_packet)\n",
    "        return packet_size_tx[max(index-1,0)],index==0\n",
    "    else:\n",
    "        index = packet_size_no_tx.index(prev_packet)\n",
    "        return packet_size_no_tx[max(index-1,0)],index==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIONS --- 0 - do nothing, 1 - increase active timer, 2 - decrease active timer\n",
    "\n",
    "# tx Index(['energyConsumption', 'ms', 'tx_power_median', 'ECL_median',\n",
    "#        'Location Coverage', 'energyConsumptionTx', 'msTx', 'duration',\n",
    "#        'interval', 'psize', 'active_timer', 'tau', 'Throughput'],\n",
    "#       dtype='object')\n",
    "# no tx Index(['energyConsumption', 'ms', 'current_max', 'ECL_median',\n",
    "#        'Location Coverage', 'duration', 'interval', 'psize', 'active_timer',\n",
    "#        'tau'],\n",
    "\n",
    "do_nothing = torch.tensor([1,0,0,0,0]).float()\n",
    "increase_timer = torch.tensor([0,1,0,0,0]).float()\n",
    "decrease_timer = torch.tensor([0,0,1,0,0]).float()\n",
    "increase_packet_size = torch.tensor([0,0,0,1,0]).float()\n",
    "decrease_packet_size = torch.tensor([0,0,0,0,1]).float()\n",
    "\n",
    "def Take_Action(current,output,predictor,scaler,with_tx):\n",
    "    if with_tx:\n",
    "        if torch.equal(output,do_nothing):\n",
    "            return current,False\n",
    "        elif torch.equal(output,increase_packet_size):#\n",
    "            new_packet,terminal = Increase_Packet_size(current[9],with_tx)\n",
    "            current[9] = new_packet\n",
    "        elif torch.equal(output,decrease_packet_size):\n",
    "            new_packet,terminal = Decrease_Packet_size(current[9],with_tx)\n",
    "            current[9] = new_packet\n",
    "        elif torch.equal(output,increase_timer):\n",
    "            new_timer,terminal = Increase_Timer(current[10],with_tx)\n",
    "            current[10] = new_timer\n",
    "        else:\n",
    "            new_timer,terminal = Decrease_Timer(current[10],with_tx)\n",
    "            current[10] = new_timer\n",
    "        \n",
    "        next_s = current\n",
    "        #drop ec,ms,tp\n",
    "        pre_predicted_features = np.delete(current,[0,1,12])\n",
    "        #input features into predictor\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(torch.Tensor(pre_predicted_features))\n",
    "        #append predicted into new list\n",
    "        next_s[0] = pre_predicted_features[0]\n",
    "        next_s[1] = pre_predicted_features[1]\n",
    "        next_s[12] = pre_predicted_features[2]\n",
    "        #return list\n",
    "        return next_s,terminal\n",
    "           \n",
    "    else:\n",
    "        if torch.equal(output,do_nothing):\n",
    "            return current,False\n",
    "        elif torch.equal(output,increase_packet_size):\n",
    "            new_packet,terminal = Increase_Packet_size(current[7],with_tx)\n",
    "            current[7] = new_packet\n",
    "        elif torch.equal(output,decrease_packet_size):\n",
    "            new_packet,terminal = Decrease_Packet_size(current[7],with_tx)\n",
    "            current[7] = new_packet\n",
    "        elif torch.equal(output,increase_timer):\n",
    "            new_timer,terminal = Increase_Timer(current[8],with_tx)\n",
    "            current[8] = new_timer\n",
    "        else:\n",
    "            new_timer,terminal = Decrease_Timer(current[8],with_tx)\n",
    "            current[8] = new_timer\n",
    "       \n",
    "        next_s = current\n",
    "        #drop ec,ms,tp\n",
    "        pre_predicted_features = np.delete(next_s,[0,1])\n",
    "        #Normalise pre predicted features into the same normalisation scale\n",
    "#         pre_predicted_features = scaler.transform(pre_predicted_features)\n",
    "        #input features into predictor\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(torch.Tensor(pre_predicted_features))\n",
    "        #append predicted into new list\n",
    "        next_s[0] = pre_predicted_features[0]\n",
    "        next_s[1] = pre_predicted_features[1]\n",
    "        #return list\n",
    "        return next_s,terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER PARAMETERS\n",
    "\n",
    "MUTATION_RATE = 0.1\n",
    "N_POP = 8\n",
    "N_GEN = 10000\n",
    "N_KEEP = int(0.5 * N_POP)\n",
    "NUMBER_OF_OUTPUTS = 5\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "ELITISM = True\n",
    "NUMBER_OF_SIM = 7\n",
    "INITIAL_EPSILON = 1\n",
    "FINAL_EPSILON = 0.01\n",
    "EPSILON_DECAY = 25000\n",
    "GAMMA = 0.9\n",
    "MINIBATCH_SIZE = 7\n",
    "\n",
    "train_data_tx,_ = Dataset(True)\n",
    "train_data_no_tx,_ = Dataset(False)\n",
    "\n",
    "NUMBER_OF_INPUTS_TX = train_data_tx.shape[1]\n",
    "NUMBER_OF_INPUTS_NO_TX = train_data_no_tx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "\n",
    "def Generate_Init_Pop(number_of_population,with_tx):\n",
    "    \n",
    "    agents = []\n",
    "    \n",
    "    for counter in range(number_of_population):\n",
    "        \n",
    "        if with_tx:\n",
    "            agent = RL_GA(NUMBER_OF_INPUTS_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)\n",
    "        else:\n",
    "            agent = RL_GA(NUMBER_OF_INPUTS_NO_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)\n",
    "\n",
    "        agents.append(agent.apply(init_weights))\n",
    "    \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_reward(model,replay_memory,NUMBER_OF_INPUTS):\n",
    "    minibatch = random.sample(replay_memory, min(len(replay_memory), MINIBATCH_SIZE))\n",
    "\n",
    "    current_batch = torch.zeros(len(minibatch),NUMBER_OF_INPUTS).to(device)\n",
    "    action_batch = torch.zeros(len(minibatch),NUMBER_OF_OUTPUTS).to(device)\n",
    "    reward_batch = torch.zeros(len(minibatch)).to(device)\n",
    "    next_state_batch = torch.zeros(len(minibatch),NUMBER_OF_INPUTS).to(device)\n",
    "    terminal_state_batch = []\n",
    "    for idx,data_point in enumerate(minibatch):\n",
    "        current_batch[idx] = data_point[0]\n",
    "        action_batch[idx] = data_point[1]\n",
    "        reward_batch[idx] = data_point[2]\n",
    "        next_state_batch[idx] = data_point[3]\n",
    "        terminal_state_batch.append(data_point[4])\n",
    "\n",
    "    next_state_batch_output = torch.zeros(MINIBATCH_SIZE,NUMBER_OF_INPUTS).to(device)\n",
    "    for idx in range(next_state_batch.shape[0]):\n",
    "        next_state_batch_output[idx] = model(next_state_batch[idx]).to(device)[0]\n",
    "\n",
    "    y_batch = tuple(reward_batch[i] if terminal_state_batch[i]\n",
    "                        else reward_batch[i] + GAMMA * torch.max(next_state_batch_output[i])\n",
    "                              for i in range(len(minibatch)))\n",
    "\n",
    "    return y_batch,current_batch,action_batch,reward_batch,next_state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Agents(agents,dataset,with_tx,target_kpi):\n",
    "    \n",
    "    predictor,scaler = load_predictor(with_tx)\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    learn_step_counter = 0\n",
    "    agent_reward = []\n",
    "    criterion = nn.MSELoss()\n",
    "    replay_memory = []\n",
    "    \n",
    "    for counter,agent in enumerate(agents):\n",
    "        rewards = []\n",
    "        for index,attribute in enumerate(dataset): \n",
    "            avg_reward = []\n",
    "            learn_step_counter += 1\n",
    "            action = agent(torch.Tensor(attribute).to(device))\n",
    "            action = Random_Action(action,epsilon)\n",
    "            next_state,terminal = Take_Action(attribute,action,predictor,scaler,with_tx)\n",
    "            reward,terminal = CalculateReward(next_state,0,target_kpi,with_tx,terminal)\n",
    "            for simulated_env in range(NUMBER_OF_SIM):\n",
    "                next_action = agent(torch.Tensor(next_state).to(device))\n",
    "                next_action = Random_Action(next_action,epsilon)\n",
    "                next_state,terminal = Take_Action(next_state,next_action,predictor,scaler,with_tx)\n",
    "                reward,terminal = CalculateReward(next_state,simulated_env,target_kpi,with_tx,terminal)\n",
    "                epsilon = FINAL_EPSILON + (INITIAL_EPSILON - FINAL_EPSILON) * math.exp(-1. * learn_step_counter / EPSILON_DECAY)\n",
    "                \n",
    "                replay_memory.append((torch.Tensor(attribute).to(device), torch.Tensor(next_action).to(device), reward, torch.Tensor(next_state).to(device), terminal))\n",
    "\n",
    "                y_batch,current_batch,action_batch,reward_batch,next_state_batch = minibatch_reward(agent,replay_memory,dataset.shape[1])\n",
    "                \n",
    "                y_batch = torch.Tensor(y_batch).detach().to(device)\n",
    "#                 extract Q-value\n",
    "                q_value = torch.sum(agent(current_batch) * action_batch, dim=1)\n",
    "                \n",
    "                loss = criterion(q_value,y_batch)\n",
    "                avg_reward.append(loss.item())\n",
    "                \n",
    "                if terminal:\n",
    "                    break\n",
    "            rewards.append(min(avg_reward))\n",
    "#         print(\"Agent: [{}/{}], Average Reward: {}\".format(counter,len(agents),rewards[counter]))\n",
    "        agent_reward.append((agent,min(rewards)))\n",
    "    return agent_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mate_Agents(agents,with_tx):\n",
    "    # Using Top to Bottom Mating\n",
    "    \n",
    "    children = []\n",
    "\n",
    "    for counter in range(0, len(agents), 2):\n",
    "        parent_1 = agents[counter]\n",
    "        parent_2 = agents[counter+1]\n",
    "        if with_tx:\n",
    "            child_1 = RL_GA(NUMBER_OF_INPUTS_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)\n",
    "            child_2 = RL_GA(NUMBER_OF_INPUTS_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)            \n",
    "        else:\n",
    "            child_1 = RL_GA(NUMBER_OF_INPUTS_NO_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)\n",
    "            child_2 = RL_GA(NUMBER_OF_INPUTS_NO_TX,HIDDEN_LAYER_SIZE,NUMBER_OF_OUTPUTS).to(device)\n",
    "\n",
    "        for index,(name,param) in enumerate(parent_1.named_parameters()):\n",
    "            if \"weight\" in name:\n",
    "                counter = int(name.split(\".\")[1])\n",
    "                parent_2_weight = parent_2.fc[counter].weight\n",
    "                child_1.fc[counter].weight = nn.Parameter(torch.cat((parent_2_weight[:round(parent_2_weight.shape[0]/2)],param[round(param.shape[0]/2):])))\n",
    "                child_2.fc[counter].weight = nn.Parameter(torch.cat((param[:round(param.shape[0]/2)],parent_2_weight[round(parent_2_weight.shape[0]/2):])))\n",
    "#             if \"bias\" in name:\n",
    "#                 counter = int(name.split(\".\")[1])\n",
    "#                 parent_2_bias = parent_2.fc[counter].bias\n",
    "#                 child_1.fc[counter].bias = nn.Parameter(torch.cat((parent_2_bias[:round(parent_2_bias.shape[0]/2)],param[round(param.shape[0]/2):])))\n",
    "#                 child_2.fc[counter].bias = nn.Parameter(torch.cat((param[:round(param.shape[0]/2)],parent_2_bias[round(parent_2_bias.shape[0]/2):])))\n",
    "        children.append(child_1)\n",
    "        children.append(child_2)\n",
    "    return agents + children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "def Mutate_Agents(agents):\n",
    "    #Elitism implemented \n",
    "    \n",
    "    number_of_mutations_0 = round(MUTATION_RATE * N_POP * agents[0].fc[0].weight.shape[1])\n",
    "    \n",
    "    for i in range(number_of_mutations_0):\n",
    "        agent = agents[random.randint(2,len(agents)-1)]\n",
    "        row = random.randint(1,agent.fc[0].weight.shape[0]-1)\n",
    "        col = random.randint(1,agent.fc[0].weight.shape[1]-1)\n",
    "        agent.fc[0].weight[row,col] = agent.fc[0].weight[row,col]+gaussian(np.mean(agent.fc[0].weight.cpu().detach().numpy()), np.std(agent.fc[0].weight.cpu().detach().numpy()), 1)\n",
    "        \n",
    "    number_of_mutations_1 = round(MUTATION_RATE * N_POP * agents[0].fc[2].weight.shape[1])\n",
    "    \n",
    "    for i in range(number_of_mutations_0):\n",
    "        agent = agents[random.randint(1,len(agents)-1)]\n",
    "        row = random.randint(1,agent.fc[2].weight.shape[0]-1)\n",
    "        col = random.randint(1,agent.fc[2].weight.shape[1]-1)\n",
    "        agent.fc[2].weight[row,col] = agent.fc[2].weight[row,col]+gaussian(np.mean(agent.fc[2].weight.cpu().detach().numpy()), np.std(agent.fc[2].weight.cpu().detach().numpy()), 1)\n",
    "    \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_Iteration(with_tx,dataset,target_kpi):\n",
    "    \n",
    "    best_agent = None\n",
    "    best_score = 99999\n",
    "    loss_history = []\n",
    "    best_gen = 0\n",
    "    \n",
    "    torch.set_grad_enabled(False) #wont be using gradient descent but using GA \n",
    "    \n",
    "    #Get Initial Population (Randomly Created Models)\n",
    "    agents = Generate_Init_Pop(N_POP,with_tx)\n",
    "                \n",
    "    for generation in range(N_GEN):\n",
    "        \n",
    "        #Run the agents and compute the rewards obtained from each agent (Cost Function)\n",
    "        agent_reward = Run_Agents(agents,dataset,with_tx,target_kpi)\n",
    "        \n",
    "        #Sort agents based on their rewards, highest reward being the best agent\n",
    "        agents = sorted(agent_reward,key=lambda x:x[1])\n",
    "        rewards = list(zip(*agents))[1]\n",
    "        agents = list(zip(*agents))[0]\n",
    "        \n",
    "        print(\"Generation: [{}/{}], Top 3 agent rewards: {}, Mean of Top Loss: {}, Best Score: {}, at: {}\".format(generation,N_GEN,rewards[:3],round(np.mean(rewards[:3]),4),round(best_score,4),best_gen))\n",
    "        \n",
    "        loss_history.append((generation,rewards[0]))\n",
    "        \n",
    "        if rewards[0] < best_score:\n",
    "            best_agent = agents[0]\n",
    "            best_score = rewards[0]\n",
    "            best_gen = generation\n",
    "            save_model(best_agent,with_tx,target_kpi)\n",
    "        \n",
    "        #Kill population that is below N_Keep (Survival of the fittest)\n",
    "        agents = list(agents[:N_KEEP])\n",
    "        \n",
    "        #Cross-over agents to obtain new children agents\n",
    "        children_agents = Mate_Agents(agents,with_tx)\n",
    "        \n",
    "        #Mutate Agents to traverse solution space\n",
    "        mutated_agents = Mutate_Agents(children_agents)\n",
    "        \n",
    "        agents = mutated_agents\n",
    "        \n",
    "        #Termination Condition, if best agent hasnt changed for 50 generations\n",
    "        if generation - best_gen >= 50:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.00955880805850029, 0.13393600285053253, 0.16043789684772491), Mean of Top Loss: 0.1013, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.02236800268292427, 0.06808505207300186, 0.08423421531915665), Mean of Top Loss: 0.0582, Best Score: 0.0096, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.0041152099147439, 0.017293497920036316, 0.1798984557390213), Mean of Top Loss: 0.0671, Best Score: 0.0096, at: 0\n",
      "Generation: [3/10000], Top 3 agent rewards: (9.447178308619186e-05, 0.010647214949131012, 0.03131796419620514), Mean of Top Loss: 0.014, Best Score: 0.0041, at: 2\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.0013452802086248994, 0.009971157647669315, 0.011739520356059074), Mean of Top Loss: 0.0077, Best Score: 0.0001, at: 3\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.0004413676797412336, 0.002181324874982238, 0.0035412886645644903), Mean of Top Loss: 0.0021, Best Score: 0.0001, at: 3\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.00038010493153706193, 0.012779186479747295, 0.013582163490355015), Mean of Top Loss: 0.0089, Best Score: 0.0001, at: 3\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.00013929525448475033, 0.002495536347851157, 0.0085165835916996), Mean of Top Loss: 0.0037, Best Score: 0.0001, at: 3\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.00025256990920752287, 0.002507168799638748, 0.003490712493658066), Mean of Top Loss: 0.0021, Best Score: 0.0001, at: 3\n",
      "Generation: [9/10000], Top 3 agent rewards: (0.00022189119772519916, 0.0038843208458274603, 0.004235479049384594), Mean of Top Loss: 0.0028, Best Score: 0.0001, at: 3\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0003772992640733719, 0.0008186445338651538, 0.004550194833427668), Mean of Top Loss: 0.0019, Best Score: 0.0001, at: 3\n",
      "Generation: [11/10000], Top 3 agent rewards: (7.049552368698642e-05, 0.00021042791195213795, 0.0020094863139092922), Mean of Top Loss: 0.0008, Best Score: 0.0001, at: 3\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.00012656346370931715, 0.00046048013609834015, 0.0022141896188259125), Mean of Top Loss: 0.0009, Best Score: 0.0001, at: 11\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.00014597004337701946, 0.0010762640740722418, 0.001393349259160459), Mean of Top Loss: 0.0009, Best Score: 0.0001, at: 11\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.0001568235456943512, 0.0017903161933645606, 0.005376946646720171), Mean of Top Loss: 0.0024, Best Score: 0.0001, at: 11\n",
      "Generation: [15/10000], Top 3 agent rewards: (0.0002549782220739871, 0.0004918084014207125, 0.00646295165643096), Mean of Top Loss: 0.0024, Best Score: 0.0001, at: 11\n",
      "Generation: [16/10000], Top 3 agent rewards: (4.786152931046672e-05, 0.011740303598344326, 0.024356279522180557), Mean of Top Loss: 0.012, Best Score: 0.0001, at: 11\n",
      "Generation: [17/10000], Top 3 agent rewards: (0.00010240212577627972, 0.005341643933206797, 0.009055107831954956), Mean of Top Loss: 0.0048, Best Score: 0.0, at: 16\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.00012872590741608292, 0.006367471069097519, 0.009400472044944763), Mean of Top Loss: 0.0053, Best Score: 0.0, at: 16\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.010946040041744709, 0.016516827046871185, 0.04989834502339363), Mean of Top Loss: 0.0258, Best Score: 0.0, at: 16\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.004433602560311556, 0.008794732391834259, 0.029834529384970665), Mean of Top Loss: 0.0144, Best Score: 0.0, at: 16\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.004433602560311556, 0.011616126634180546, 0.017193008214235306), Mean of Top Loss: 0.0111, Best Score: 0.0, at: 16\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.0018487690249457955, 0.0038979947566986084, 0.013300632126629353), Mean of Top Loss: 0.0063, Best Score: 0.0, at: 16\n",
      "Generation: [23/10000], Top 3 agent rewards: (0.0031062413472682238, 0.004887967836111784, 0.0064913383685052395), Mean of Top Loss: 0.0048, Best Score: 0.0, at: 16\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.003033329965546727, 0.005042862147092819, 0.00909269880503416), Mean of Top Loss: 0.0057, Best Score: 0.0, at: 16\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.008402465842664242, 0.015158483758568764, 0.01518642995506525), Mean of Top Loss: 0.0129, Best Score: 0.0, at: 16\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.0014144880697131157, 0.006431829649955034, 0.03935883566737175), Mean of Top Loss: 0.0157, Best Score: 0.0, at: 16\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.0012218820629641414, 0.005050144623965025, 0.007252891082316637), Mean of Top Loss: 0.0045, Best Score: 0.0, at: 16\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.006308198440819979, 0.010469287633895874, 0.020085617899894714), Mean of Top Loss: 0.0123, Best Score: 0.0, at: 16\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0004894519806839526, 0.018319617956876755, 0.026419702917337418), Mean of Top Loss: 0.0151, Best Score: 0.0, at: 16\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.0027942785527557135, 0.007094011642038822, 0.019577469676733017), Mean of Top Loss: 0.0098, Best Score: 0.0, at: 16\n",
      "Generation: [31/10000], Top 3 agent rewards: (0.006616119761019945, 0.022959044203162193, 0.02370116487145424), Mean of Top Loss: 0.0178, Best Score: 0.0, at: 16\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.0061616478487849236, 0.006812885403633118, 0.043345920741558075), Mean of Top Loss: 0.0188, Best Score: 0.0, at: 16\n",
      "Generation: [33/10000], Top 3 agent rewards: (0.0018030538922175765, 0.009684189222753048, 0.026311544701457024), Mean of Top Loss: 0.0126, Best Score: 0.0, at: 16\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.0030441160779446363, 0.026233913376927376, 0.02928544394671917), Mean of Top Loss: 0.0195, Best Score: 0.0, at: 16\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.0026802208740264177, 0.048859626054763794, 0.16503669321537018), Mean of Top Loss: 0.0722, Best Score: 0.0, at: 16\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.0017329727998003364, 0.0025909102987498045, 0.027850408107042313), Mean of Top Loss: 0.0107, Best Score: 0.0, at: 16\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.060440097004175186, 0.14626821875572205, 0.23057842254638672), Mean of Top Loss: 0.1458, Best Score: 0.0, at: 16\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.016841521486639977, 0.09095364809036255, 0.11312106996774673), Mean of Top Loss: 0.0736, Best Score: 0.0, at: 16\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.01816922053694725, 0.13707880675792694, 0.23425722122192383), Mean of Top Loss: 0.1298, Best Score: 0.0, at: 16\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.361502468585968, 0.5540818572044373, 0.70347660779953), Mean of Top Loss: 0.5397, Best Score: 0.0, at: 16\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.03337419778108597, 0.15162798762321472, 0.19011762738227844), Mean of Top Loss: 0.125, Best Score: 0.0, at: 16\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.003681675298139453, 0.17923101782798767, 0.22598637640476227), Mean of Top Loss: 0.1363, Best Score: 0.0, at: 16\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.06504441052675247, 0.34819525480270386, 0.45475393533706665), Mean of Top Loss: 0.2893, Best Score: 0.0, at: 16\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.04486924037337303, 0.09152480214834213, 0.30588552355766296), Mean of Top Loss: 0.1474, Best Score: 0.0, at: 16\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.05796337127685547, 0.330695778131485, 0.3672932982444763), Mean of Top Loss: 0.252, Best Score: 0.0, at: 16\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.3725225627422333, 0.6189279556274414, 0.8420754671096802), Mean of Top Loss: 0.6112, Best Score: 0.0, at: 16\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.1445728987455368, 0.17854373157024384, 0.588989794254303), Mean of Top Loss: 0.304, Best Score: 0.0, at: 16\n",
      "Generation: [48/10000], Top 3 agent rewards: (1.4714736938476562, 2.096747636795044, 2.3524169921875), Mean of Top Loss: 1.9735, Best Score: 0.0, at: 16\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.05806102231144905, 0.18844588100910187, 0.19333863258361816), Mean of Top Loss: 0.1466, Best Score: 0.0, at: 16\n",
      "Generation: [50/10000], Top 3 agent rewards: (0.3490672707557678, 0.39070940017700195, 0.6474804282188416), Mean of Top Loss: 0.4624, Best Score: 0.0, at: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [51/10000], Top 3 agent rewards: (0.5773151516914368, 0.641319215297699, 0.8611899018287659), Mean of Top Loss: 0.6933, Best Score: 0.0, at: 16\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.3194562494754791, 0.3664868175983429, 0.5138153433799744), Mean of Top Loss: 0.3999, Best Score: 0.0, at: 16\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.07161099463701248, 0.08189620077610016, 0.15107761323451996), Mean of Top Loss: 0.1015, Best Score: 0.0, at: 16\n",
      "Generation: [54/10000], Top 3 agent rewards: (0.1975373476743698, 0.8276351094245911, 0.9249558448791504), Mean of Top Loss: 0.65, Best Score: 0.0, at: 16\n",
      "Generation: [55/10000], Top 3 agent rewards: (0.16594494879245758, 0.26647067070007324, 0.3981192409992218), Mean of Top Loss: 0.2768, Best Score: 0.0, at: 16\n",
      "Generation: [56/10000], Top 3 agent rewards: (0.7477744221687317, 2.0062832832336426, 3.5762059688568115), Mean of Top Loss: 2.1101, Best Score: 0.0, at: 16\n",
      "Generation: [57/10000], Top 3 agent rewards: (0.24011847376823425, 0.5616737604141235, 0.5911377668380737), Mean of Top Loss: 0.4643, Best Score: 0.0, at: 16\n",
      "Generation: [58/10000], Top 3 agent rewards: (0.0271986685693264, 0.3911122679710388, 1.2656112909317017), Mean of Top Loss: 0.5613, Best Score: 0.0, at: 16\n",
      "Generation: [59/10000], Top 3 agent rewards: (0.5779743194580078, 0.7173636555671692, 0.9198702573776245), Mean of Top Loss: 0.7384, Best Score: 0.0, at: 16\n",
      "Generation: [60/10000], Top 3 agent rewards: (0.35481786727905273, 0.4367370307445526, 0.4758475720882416), Mean of Top Loss: 0.4225, Best Score: 0.0, at: 16\n",
      "Generation: [61/10000], Top 3 agent rewards: (0.16203947365283966, 0.3266899883747101, 0.42897388339042664), Mean of Top Loss: 0.3059, Best Score: 0.0, at: 16\n",
      "Generation: [62/10000], Top 3 agent rewards: (0.6788592338562012, 0.8689054846763611, 1.0034974813461304), Mean of Top Loss: 0.8504, Best Score: 0.0, at: 16\n",
      "Generation: [63/10000], Top 3 agent rewards: (0.3030087649822235, 0.5847806930541992, 0.6068459749221802), Mean of Top Loss: 0.4982, Best Score: 0.0, at: 16\n",
      "Generation: [64/10000], Top 3 agent rewards: (0.3288200795650482, 0.7120195627212524, 0.7980111241340637), Mean of Top Loss: 0.613, Best Score: 0.0, at: 16\n",
      "Generation: [65/10000], Top 3 agent rewards: (0.22446754574775696, 0.2515880763530731, 0.9239338040351868), Mean of Top Loss: 0.4667, Best Score: 0.0, at: 16\n",
      "Generation: [66/10000], Top 3 agent rewards: (0.5489631295204163, 0.6057090163230896, 0.8255005478858948), Mean of Top Loss: 0.6601, Best Score: 0.0, at: 16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(True,train_data_tx,\"all\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"tx\",\"GA\",\"all\"), index=False)\n",
    "\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.0005685550277121365, 0.0014823154779151082, 0.0033670682460069656), Mean of Top Loss: 0.0018, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.000858251063618809, 0.0027297288179397583, 0.043075162917375565), Mean of Top Loss: 0.0156, Best Score: 0.0006, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.01971781626343727, 0.03891737014055252, 0.06673352420330048), Mean of Top Loss: 0.0418, Best Score: 0.0006, at: 0\n",
      "Generation: [3/10000], Top 3 agent rewards: (1.3502861975212e-06, 0.00048196688294410706, 0.0006347471498884261), Mean of Top Loss: 0.0004, Best Score: 0.0006, at: 0\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.0012367324670776725, 0.00186972098890692, 0.009794795885682106), Mean of Top Loss: 0.0043, Best Score: 0.0, at: 3\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.0010687434114515781, 0.002557795960456133, 0.00959065742790699), Mean of Top Loss: 0.0044, Best Score: 0.0, at: 3\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.0006907894276082516, 0.0010912836296483874, 0.0016623558476567268), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 3\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.0006278179353103042, 0.0026637129485607147, 0.0036402803380042315), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 3\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.0007227595779113472, 0.00936959683895111, 0.01001437846571207), Mean of Top Loss: 0.0067, Best Score: 0.0, at: 3\n",
      "Generation: [9/10000], Top 3 agent rewards: (0.0003036776033695787, 0.004767674021422863, 0.022815968841314316), Mean of Top Loss: 0.0093, Best Score: 0.0, at: 3\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.004813686944544315, 0.013070127926766872, 0.01499370951205492), Mean of Top Loss: 0.011, Best Score: 0.0, at: 3\n",
      "Generation: [11/10000], Top 3 agent rewards: (0.003962074406445026, 0.004571743309497833, 0.005238709039986134), Mean of Top Loss: 0.0046, Best Score: 0.0, at: 3\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.001609160448424518, 0.0037882402539253235, 0.004554205574095249), Mean of Top Loss: 0.0033, Best Score: 0.0, at: 3\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.0011974142398685217, 0.009383701719343662, 0.009857931174337864), Mean of Top Loss: 0.0068, Best Score: 0.0, at: 3\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.0012205837992951274, 0.0016448424430564046, 0.016081849113106728), Mean of Top Loss: 0.0063, Best Score: 0.0, at: 3\n",
      "Generation: [15/10000], Top 3 agent rewards: (0.0038443978410214186, 0.0045999991707503796, 0.005310155916959047), Mean of Top Loss: 0.0046, Best Score: 0.0, at: 3\n",
      "Generation: [16/10000], Top 3 agent rewards: (0.0001171490439446643, 0.00027702772058546543, 0.003068452002480626), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 3\n",
      "Generation: [17/10000], Top 3 agent rewards: (3.276391726103611e-05, 6.870704964967445e-05, 0.0023874863982200623), Mean of Top Loss: 0.0008, Best Score: 0.0, at: 3\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.009480713866651058, 0.01131656114012003, 0.029574697837233543), Mean of Top Loss: 0.0168, Best Score: 0.0, at: 3\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.005199805833399296, 0.015010020695626736, 0.036648426204919815), Mean of Top Loss: 0.019, Best Score: 0.0, at: 3\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.003577197203412652, 0.015053586103022099, 0.042511146515607834), Mean of Top Loss: 0.0204, Best Score: 0.0, at: 3\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.016068793833255768, 0.016451282426714897, 0.02986985817551613), Mean of Top Loss: 0.0208, Best Score: 0.0, at: 3\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.011189950630068779, 0.012387341819703579, 0.02268316224217415), Mean of Top Loss: 0.0154, Best Score: 0.0, at: 3\n",
      "Generation: [23/10000], Top 3 agent rewards: (0.0027181042823940516, 0.0086349593475461, 0.019406959414482117), Mean of Top Loss: 0.0103, Best Score: 0.0, at: 3\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.0014217831194400787, 0.00246223877184093, 0.007050338666886091), Mean of Top Loss: 0.0036, Best Score: 0.0, at: 3\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.009232119657099247, 0.009868192486464977, 0.028818313032388687), Mean of Top Loss: 0.016, Best Score: 0.0, at: 3\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.024765649810433388, 0.033043622970581055, 0.07690942287445068), Mean of Top Loss: 0.0449, Best Score: 0.0, at: 3\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.02610301598906517, 0.052186284214258194, 0.08260549604892731), Mean of Top Loss: 0.0536, Best Score: 0.0, at: 3\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.02785128355026245, 0.031678758561611176, 0.061039622873067856), Mean of Top Loss: 0.0402, Best Score: 0.0, at: 3\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0260095726698637, 0.08529365062713623, 0.08717484027147293), Mean of Top Loss: 0.0662, Best Score: 0.0, at: 3\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.009111197665333748, 0.023073354735970497, 0.030972493812441826), Mean of Top Loss: 0.0211, Best Score: 0.0, at: 3\n",
      "Generation: [31/10000], Top 3 agent rewards: (0.00574324419721961, 0.017291633412241936, 0.0203235000371933), Mean of Top Loss: 0.0145, Best Score: 0.0, at: 3\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.024507401511073112, 0.028821764513850212, 0.04848788306117058), Mean of Top Loss: 0.0339, Best Score: 0.0, at: 3\n",
      "Generation: [33/10000], Top 3 agent rewards: (0.0041954405605793, 0.016575686633586884, 0.0637991651892662), Mean of Top Loss: 0.0282, Best Score: 0.0, at: 3\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.009413814172148705, 0.04550671577453613, 0.04551910236477852), Mean of Top Loss: 0.0335, Best Score: 0.0, at: 3\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.00398503290489316, 0.0550856776535511, 0.05570080503821373), Mean of Top Loss: 0.0383, Best Score: 0.0, at: 3\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.0010869131656363606, 0.016592061147093773, 0.019847068935632706), Mean of Top Loss: 0.0125, Best Score: 0.0, at: 3\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.001409142161719501, 0.0057312785647809505, 0.006497965659946203), Mean of Top Loss: 0.0045, Best Score: 0.0, at: 3\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.0012808818137273192, 0.00412377854809165, 0.007147057913243771), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 3\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.0060570454224944115, 0.02595999464392662, 0.0388081856071949), Mean of Top Loss: 0.0236, Best Score: 0.0, at: 3\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.012297911569476128, 0.044722478836774826, 0.04981797933578491), Mean of Top Loss: 0.0356, Best Score: 0.0, at: 3\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.03535234183073044, 0.04276087507605553, 0.04612260311841965), Mean of Top Loss: 0.0414, Best Score: 0.0, at: 3\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.022174140438437462, 0.04250142723321915, 0.047739118337631226), Mean of Top Loss: 0.0375, Best Score: 0.0, at: 3\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.030933156609535217, 0.03976504132151604, 0.043889980763196945), Mean of Top Loss: 0.0382, Best Score: 0.0, at: 3\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.006465440150350332, 0.006702779792249203, 0.02796955406665802), Mean of Top Loss: 0.0137, Best Score: 0.0, at: 3\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.016752565279603004, 0.01813310757279396, 0.036872606724500656), Mean of Top Loss: 0.0239, Best Score: 0.0, at: 3\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.021189024671912193, 0.02328641526401043, 0.059479691088199615), Mean of Top Loss: 0.0347, Best Score: 0.0, at: 3\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.029790090397000313, 0.044382087886333466, 0.05333111807703972), Mean of Top Loss: 0.0425, Best Score: 0.0, at: 3\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.11315103620290756, 0.12056364119052887, 0.1506703644990921), Mean of Top Loss: 0.1281, Best Score: 0.0, at: 3\n",
      "Generation: [49/10000], Top 3 agent rewards: (2.7941514417761937e-05, 0.044263310730457306, 0.056741926819086075), Mean of Top Loss: 0.0337, Best Score: 0.0, at: 3\n",
      "Generation: [50/10000], Top 3 agent rewards: (0.035398658365011215, 0.07281417399644852, 0.15098753571510315), Mean of Top Loss: 0.0864, Best Score: 0.0, at: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [51/10000], Top 3 agent rewards: (0.05316464230418205, 0.0597582645714283, 0.0913124829530716), Mean of Top Loss: 0.0681, Best Score: 0.0, at: 3\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.047770414501428604, 0.07087042182683945, 0.09555792808532715), Mean of Top Loss: 0.0714, Best Score: 0.0, at: 3\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.05242278054356575, 0.06976612657308578, 0.07703100144863129), Mean of Top Loss: 0.0664, Best Score: 0.0, at: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(True,train_data_tx,\"tp\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"tx\",\"GA\",\"tp\"), index=False)\n",
    "\n",
    "#DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.0004212133935652673, 0.0009268896537832916, 0.03469941020011902), Mean of Top Loss: 0.012, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.012260169722139835, 0.020197059959173203, 0.022891070693731308), Mean of Top Loss: 0.0184, Best Score: 0.0004, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.0013713202206417918, 0.012684623710811138, 0.019196510314941406), Mean of Top Loss: 0.0111, Best Score: 0.0004, at: 0\n",
      "Generation: [3/10000], Top 3 agent rewards: (0.0017712869448587298, 0.0027552300598472357, 0.002968516666442156), Mean of Top Loss: 0.0025, Best Score: 0.0004, at: 0\n",
      "Generation: [4/10000], Top 3 agent rewards: (4.091403025086038e-05, 0.00031262196716852486, 0.000598990241996944), Mean of Top Loss: 0.0003, Best Score: 0.0004, at: 0\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.00032538044615648687, 0.0018606233643367887, 0.002461639465764165), Mean of Top Loss: 0.0015, Best Score: 0.0, at: 4\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.0011802100343629718, 0.0014050285099074244, 0.0044240802526474), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 4\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.00045783264795318246, 0.00100637914147228, 0.0029887824784964323), Mean of Top Loss: 0.0015, Best Score: 0.0, at: 4\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.0008260445902124047, 0.0009091303800232708, 0.0014894457999616861), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 4\n",
      "Generation: [9/10000], Top 3 agent rewards: (0.000515322491992265, 0.000948666303884238, 0.0022175409831106663), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 4\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0035464114043861628, 0.0046768574975430965, 0.006569843739271164), Mean of Top Loss: 0.0049, Best Score: 0.0, at: 4\n",
      "Generation: [11/10000], Top 3 agent rewards: (0.0018703798996284604, 0.0027138672303408384, 0.0035581861156970263), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 4\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.0013253388460725546, 0.005114652216434479, 0.006808015517890453), Mean of Top Loss: 0.0044, Best Score: 0.0, at: 4\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.0004172303306404501, 0.00471858587116003, 0.004893229342997074), Mean of Top Loss: 0.0033, Best Score: 0.0, at: 4\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.001023156219162047, 0.0024911437649279833, 0.0027599851600825787), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 4\n",
      "Generation: [15/10000], Top 3 agent rewards: (0.0014113138895481825, 0.0034700469113886356, 0.003498972626402974), Mean of Top Loss: 0.0028, Best Score: 0.0, at: 4\n",
      "Generation: [16/10000], Top 3 agent rewards: (0.002875650068745017, 0.004642066080123186, 0.011443430557847023), Mean of Top Loss: 0.0063, Best Score: 0.0, at: 4\n",
      "Generation: [17/10000], Top 3 agent rewards: (0.0014207782223820686, 0.004742542747408152, 0.006385121028870344), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 4\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.0002980864665005356, 0.010492672212421894, 0.013653573580086231), Mean of Top Loss: 0.0081, Best Score: 0.0, at: 4\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.0010703295702114701, 0.004104392137378454, 0.012129424139857292), Mean of Top Loss: 0.0058, Best Score: 0.0, at: 4\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.0005954141961410642, 0.0025517516769468784, 0.010030032135546207), Mean of Top Loss: 0.0044, Best Score: 0.0, at: 4\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.0014357378240674734, 0.0016661121044307947, 0.003076855791732669), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 4\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.001980984816327691, 0.002514093881472945, 0.0025210550520569086), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 4\n",
      "Generation: [23/10000], Top 3 agent rewards: (0.0014542974531650543, 0.004790753126144409, 0.016306277364492416), Mean of Top Loss: 0.0075, Best Score: 0.0, at: 4\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.0018946451600641012, 0.024749156087636948, 0.04163796082139015), Mean of Top Loss: 0.0228, Best Score: 0.0, at: 4\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.046156711876392365, 0.053567953407764435, 0.05752410739660263), Mean of Top Loss: 0.0524, Best Score: 0.0, at: 4\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.015405005775392056, 0.04040228947997093, 0.0508500337600708), Mean of Top Loss: 0.0356, Best Score: 0.0, at: 4\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.008495432324707508, 0.01040475070476532, 0.016213353723287582), Mean of Top Loss: 0.0117, Best Score: 0.0, at: 4\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.008537213318049908, 0.029992470517754555, 0.040386080741882324), Mean of Top Loss: 0.0263, Best Score: 0.0, at: 4\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0005209483206272125, 0.05604181066155434, 0.14110569655895233), Mean of Top Loss: 0.0659, Best Score: 0.0, at: 4\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.0072678616270422935, 0.03276379033923149, 0.05174652114510536), Mean of Top Loss: 0.0306, Best Score: 0.0, at: 4\n",
      "Generation: [31/10000], Top 3 agent rewards: (0.008741052821278572, 0.037571363151073456, 0.1236218735575676), Mean of Top Loss: 0.0566, Best Score: 0.0, at: 4\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.009136131964623928, 0.04686442017555237, 0.10548441857099533), Mean of Top Loss: 0.0538, Best Score: 0.0, at: 4\n",
      "Generation: [33/10000], Top 3 agent rewards: (0.0065342215821146965, 0.020224090665578842, 0.03827846050262451), Mean of Top Loss: 0.0217, Best Score: 0.0, at: 4\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.010295446030795574, 0.012140179984271526, 0.040196869522333145), Mean of Top Loss: 0.0209, Best Score: 0.0, at: 4\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.009660889394581318, 0.048999469727277756, 0.35464245080947876), Mean of Top Loss: 0.1378, Best Score: 0.0, at: 4\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.005611791275441647, 0.018535396084189415, 0.030774977058172226), Mean of Top Loss: 0.0183, Best Score: 0.0, at: 4\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.008293164893984795, 0.009415916167199612, 0.021594787016510963), Mean of Top Loss: 0.0131, Best Score: 0.0, at: 4\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.005343222059309483, 0.013223244808614254, 0.015857143327593803), Mean of Top Loss: 0.0115, Best Score: 0.0, at: 4\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.01054819393903017, 0.014733357354998589, 0.050940848886966705), Mean of Top Loss: 0.0254, Best Score: 0.0, at: 4\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.00348089262843132, 0.009536800906062126, 0.021700343117117882), Mean of Top Loss: 0.0116, Best Score: 0.0, at: 4\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.0038446246180683374, 0.04571039229631424, 0.08180954307317734), Mean of Top Loss: 0.0438, Best Score: 0.0, at: 4\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.007199277635663748, 0.01859271340072155, 0.023810897022485733), Mean of Top Loss: 0.0165, Best Score: 0.0, at: 4\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.004441487602889538, 0.017662158235907555, 0.017690526321530342), Mean of Top Loss: 0.0133, Best Score: 0.0, at: 4\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.0016580650117248297, 0.003951757214963436, 0.03930008038878441), Mean of Top Loss: 0.015, Best Score: 0.0, at: 4\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.026612788438796997, 0.03922603279352188, 0.044133029878139496), Mean of Top Loss: 0.0367, Best Score: 0.0, at: 4\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.00014241908502299339, 0.010964636690914631, 0.03470752760767937), Mean of Top Loss: 0.0153, Best Score: 0.0, at: 4\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.010772347450256348, 0.026036759838461876, 0.028582913801074028), Mean of Top Loss: 0.0218, Best Score: 0.0, at: 4\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.017089853063225746, 0.029807088896632195, 0.054721932858228683), Mean of Top Loss: 0.0339, Best Score: 0.0, at: 4\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.0327865406870842, 0.03866561874747276, 0.05567360296845436), Mean of Top Loss: 0.0424, Best Score: 0.0, at: 4\n",
      "Generation: [50/10000], Top 3 agent rewards: (0.017605500295758247, 0.028473738580942154, 0.03619125857949257), Mean of Top Loss: 0.0274, Best Score: 0.0, at: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [51/10000], Top 3 agent rewards: (0.0016297107795253396, 0.05389817804098129, 0.08333785086870193), Mean of Top Loss: 0.0463, Best Score: 0.0, at: 4\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.03647121414542198, 0.042937956750392914, 0.13056756556034088), Mean of Top Loss: 0.07, Best Score: 0.0, at: 4\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.08889137208461761, 0.1140892282128334, 0.1237921193242073), Mean of Top Loss: 0.1089, Best Score: 0.0, at: 4\n",
      "Generation: [54/10000], Top 3 agent rewards: (0.01549400296062231, 0.03312131389975548, 0.061743296682834625), Mean of Top Loss: 0.0368, Best Score: 0.0, at: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(True,train_data_tx,\"ec\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"tx\",\"GA\",\"ec\"), index=False)\n",
    "\n",
    "#Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.11482720822095871, 0.11487133055925369, 0.14755359292030334), Mean of Top Loss: 0.1258, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.006580919958651066, 0.0438915491104126, 0.07561598718166351), Mean of Top Loss: 0.042, Best Score: 0.1148, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.0030403246637433767, 0.022822711616754532, 0.04050659015774727), Mean of Top Loss: 0.0221, Best Score: 0.0066, at: 1\n",
      "Generation: [3/10000], Top 3 agent rewards: (5.491766614795779e-07, 0.02044663019478321, 0.02221756801009178), Mean of Top Loss: 0.0142, Best Score: 0.003, at: 2\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.000506565032992512, 0.004272902850061655, 0.005664707161486149), Mean of Top Loss: 0.0035, Best Score: 0.0, at: 3\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.0023393670562654734, 0.0024555851705372334, 0.003294486552476883), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 3\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.0002675912983249873, 0.0017739970935508609, 0.004222852177917957), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 3\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.000707977160345763, 0.00229097087867558, 0.003044014098122716), Mean of Top Loss: 0.002, Best Score: 0.0, at: 3\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.0006742535042576492, 0.0112473014742136, 0.014174596406519413), Mean of Top Loss: 0.0087, Best Score: 0.0, at: 3\n",
      "Generation: [9/10000], Top 3 agent rewards: (0.000604442844633013, 0.010677809827029705, 0.024273047223687172), Mean of Top Loss: 0.0119, Best Score: 0.0, at: 3\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0003504802007228136, 0.002049945993348956, 0.007506119552999735), Mean of Top Loss: 0.0033, Best Score: 0.0, at: 3\n",
      "Generation: [11/10000], Top 3 agent rewards: (0.0007270830101333559, 0.0009895932162180543, 0.010000908747315407), Mean of Top Loss: 0.0039, Best Score: 0.0, at: 3\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.000973629648797214, 0.00174546183552593, 0.002676556585356593), Mean of Top Loss: 0.0018, Best Score: 0.0, at: 3\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.0006123530911281705, 0.0018432148499414325, 0.0019476221641525626), Mean of Top Loss: 0.0015, Best Score: 0.0, at: 3\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.003365566022694111, 0.0044973609037697315, 0.005755220539867878), Mean of Top Loss: 0.0045, Best Score: 0.0, at: 3\n",
      "Generation: [15/10000], Top 3 agent rewards: (0.0005899723037146032, 0.007265426218509674, 0.007763482630252838), Mean of Top Loss: 0.0052, Best Score: 0.0, at: 3\n",
      "Generation: [16/10000], Top 3 agent rewards: (0.00010476703755557537, 0.0005400459049269557, 0.01633383333683014), Mean of Top Loss: 0.0057, Best Score: 0.0, at: 3\n",
      "Generation: [17/10000], Top 3 agent rewards: (0.0003749011375475675, 0.004011199343949556, 0.023817572742700577), Mean of Top Loss: 0.0094, Best Score: 0.0, at: 3\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.0003423730668146163, 0.0023344375658780336, 0.00479920906946063), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 3\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.001554385875351727, 0.00259034289047122, 0.002611996605992317), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 3\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.0006244335672818124, 0.008672801777720451, 0.009091705083847046), Mean of Top Loss: 0.0061, Best Score: 0.0, at: 3\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.00032768159871920943, 0.0056406063959002495, 0.007003481034189463), Mean of Top Loss: 0.0043, Best Score: 0.0, at: 3\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.0010996080236509442, 0.005899617914110422, 0.006139213684946299), Mean of Top Loss: 0.0044, Best Score: 0.0, at: 3\n",
      "Generation: [23/10000], Top 3 agent rewards: (0.00039725113310851157, 0.007829039357602596, 0.00929148681461811), Mean of Top Loss: 0.0058, Best Score: 0.0, at: 3\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.0011432226747274399, 0.004263077396899462, 0.0057109021581709385), Mean of Top Loss: 0.0037, Best Score: 0.0, at: 3\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.0024704462848603725, 0.002479136222973466, 0.002716632327064872), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 3\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.0009205123060382903, 0.0013938354095444083, 0.012277128174901009), Mean of Top Loss: 0.0049, Best Score: 0.0, at: 3\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.0034776569809764624, 0.004894518293440342, 0.03437300771474838), Mean of Top Loss: 0.0142, Best Score: 0.0, at: 3\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.005430702585726976, 0.005538295954465866, 0.011997828260064125), Mean of Top Loss: 0.0077, Best Score: 0.0, at: 3\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0009790885960683227, 0.002886224305257201, 0.008469285443425179), Mean of Top Loss: 0.0041, Best Score: 0.0, at: 3\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.0005021897959522903, 0.003054716158658266, 0.012215961702167988), Mean of Top Loss: 0.0053, Best Score: 0.0, at: 3\n",
      "Generation: [31/10000], Top 3 agent rewards: (0.001509481924585998, 0.010140118189156055, 0.012276039458811283), Mean of Top Loss: 0.008, Best Score: 0.0, at: 3\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.0012430122587829828, 0.0377657487988472, 0.043905872851610184), Mean of Top Loss: 0.0276, Best Score: 0.0, at: 3\n",
      "Generation: [33/10000], Top 3 agent rewards: (0.000698968768119812, 0.012911294586956501, 0.013463503681123257), Mean of Top Loss: 0.009, Best Score: 0.0, at: 3\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.000415461283409968, 0.0018449003109708428, 0.007922066375613213), Mean of Top Loss: 0.0034, Best Score: 0.0, at: 3\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.0004088692367076874, 0.001370939426124096, 0.013833299279212952), Mean of Top Loss: 0.0052, Best Score: 0.0, at: 3\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.0002813313330989331, 0.006528656464070082, 0.011702844873070717), Mean of Top Loss: 0.0062, Best Score: 0.0, at: 3\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.0006679351208731532, 0.0007722253794781864, 0.0059294081293046474), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 3\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.011318749748170376, 0.011468766257166862, 0.022178759798407555), Mean of Top Loss: 0.015, Best Score: 0.0, at: 3\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.007034045644104481, 0.008748159743845463, 0.013651381246745586), Mean of Top Loss: 0.0098, Best Score: 0.0, at: 3\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.00950407050549984, 0.011599473655223846, 0.012049732729792595), Mean of Top Loss: 0.0111, Best Score: 0.0, at: 3\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.0022141605149954557, 0.02285505272448063, 0.023529052734375), Mean of Top Loss: 0.0162, Best Score: 0.0, at: 3\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.008755192160606384, 0.009648156352341175, 0.011017395183444023), Mean of Top Loss: 0.0098, Best Score: 0.0, at: 3\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.003209812333807349, 0.031232595443725586, 0.03323472663760185), Mean of Top Loss: 0.0226, Best Score: 0.0, at: 3\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.00026760931359604, 0.01268835086375475, 0.015041844919323921), Mean of Top Loss: 0.0093, Best Score: 0.0, at: 3\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.0005318777402862906, 0.0005705360090360045, 0.011283651925623417), Mean of Top Loss: 0.0041, Best Score: 0.0, at: 3\n",
      "Generation: [46/10000], Top 3 agent rewards: (7.821595499990508e-05, 0.0034037248697131872, 0.007910303771495819), Mean of Top Loss: 0.0038, Best Score: 0.0, at: 3\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.00039919247501529753, 0.0023542086128145456, 0.011997501365840435), Mean of Top Loss: 0.0049, Best Score: 0.0, at: 3\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.00021297161583788693, 0.00047368297236971557, 0.009028906933963299), Mean of Top Loss: 0.0032, Best Score: 0.0, at: 3\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.0004949009162373841, 0.007199143059551716, 0.009155874140560627), Mean of Top Loss: 0.0056, Best Score: 0.0, at: 3\n",
      "Generation: [50/10000], Top 3 agent rewards: (0.00011875520431203768, 0.0021681757643818855, 0.006129706744104624), Mean of Top Loss: 0.0028, Best Score: 0.0, at: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [51/10000], Top 3 agent rewards: (7.821809413144365e-05, 0.006132814567536116, 0.0071581569500267506), Mean of Top Loss: 0.0045, Best Score: 0.0, at: 3\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.000758285925257951, 0.001040731673128903, 0.0036889761686325073), Mean of Top Loss: 0.0018, Best Score: 0.0, at: 3\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.0008497791714034975, 0.00372009607963264, 0.013745574280619621), Mean of Top Loss: 0.0061, Best Score: 0.0, at: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(True,train_data_tx,\"ms\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"tx\",\"GA\",\"ms\"), index=False)\n",
    "\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.08346132189035416, 0.09857472032308578, 0.20926222205162048), Mean of Top Loss: 0.1304, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.05038314312696457, 0.1040852889418602, 0.11178449541330338), Mean of Top Loss: 0.0888, Best Score: 0.0835, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.0034211284946650267, 0.01991449110209942, 0.03738447651267052), Mean of Top Loss: 0.0202, Best Score: 0.0504, at: 1\n",
      "Generation: [3/10000], Top 3 agent rewards: (0.003617102513089776, 0.02030620351433754, 0.03708525747060776), Mean of Top Loss: 0.0203, Best Score: 0.0034, at: 2\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.0011096675880253315, 0.018445078283548355, 0.01985679194331169), Mean of Top Loss: 0.0131, Best Score: 0.0034, at: 2\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.001013319124467671, 0.005611230153590441, 0.010945002548396587), Mean of Top Loss: 0.0059, Best Score: 0.0011, at: 4\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.002854326507076621, 0.0030105081386864185, 0.010117299854755402), Mean of Top Loss: 0.0053, Best Score: 0.001, at: 5\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.001343827461823821, 0.0025055930018424988, 0.014957408420741558), Mean of Top Loss: 0.0063, Best Score: 0.001, at: 5\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.002293070312589407, 0.0025600434746593237, 0.01224747858941555), Mean of Top Loss: 0.0057, Best Score: 0.001, at: 5\n",
      "Generation: [9/10000], Top 3 agent rewards: (0.0007009156397543848, 0.0017148120095953345, 0.0036509844940155745), Mean of Top Loss: 0.002, Best Score: 0.001, at: 5\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0005517060053534806, 0.004101399797946215, 0.004260098561644554), Mean of Top Loss: 0.003, Best Score: 0.0007, at: 9\n",
      "Generation: [11/10000], Top 3 agent rewards: (0.0008892383775673807, 0.0016291179927065969, 0.0037057052832096815), Mean of Top Loss: 0.0021, Best Score: 0.0006, at: 10\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.00045709830010309815, 0.005621978081762791, 0.007161230780184269), Mean of Top Loss: 0.0044, Best Score: 0.0006, at: 10\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.0009981475304812193, 0.002318992745131254, 0.004286918323487043), Mean of Top Loss: 0.0025, Best Score: 0.0005, at: 12\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.0005489449831657112, 0.006256479304283857, 0.009554200805723667), Mean of Top Loss: 0.0055, Best Score: 0.0005, at: 12\n",
      "Generation: [15/10000], Top 3 agent rewards: (0.0005390280275605619, 0.0014948485186323524, 0.006855386774986982), Mean of Top Loss: 0.003, Best Score: 0.0005, at: 12\n",
      "Generation: [16/10000], Top 3 agent rewards: (0.0009178243926726282, 0.0029814967419952154, 0.0035760418977588415), Mean of Top Loss: 0.0025, Best Score: 0.0005, at: 12\n",
      "Generation: [17/10000], Top 3 agent rewards: (8.941032137954608e-05, 0.0001195552249555476, 0.0011544221779331565), Mean of Top Loss: 0.0005, Best Score: 0.0005, at: 12\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.0002246707445010543, 0.000699551950674504, 0.007586275227367878), Mean of Top Loss: 0.0028, Best Score: 0.0001, at: 17\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.00021117103460710496, 0.0017636966658756137, 0.0035298189613968134), Mean of Top Loss: 0.0018, Best Score: 0.0001, at: 17\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.0002577232080511749, 0.0007710664649493992, 0.0022809121292084455), Mean of Top Loss: 0.0011, Best Score: 0.0001, at: 17\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.0003380838024895638, 0.009648476727306843, 0.010136781260371208), Mean of Top Loss: 0.0067, Best Score: 0.0001, at: 17\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.00012801389675587416, 0.003325172234326601, 0.004548958037048578), Mean of Top Loss: 0.0027, Best Score: 0.0001, at: 17\n",
      "Generation: [23/10000], Top 3 agent rewards: (0.00018571328837424517, 0.0037571501452475786, 0.006053929217159748), Mean of Top Loss: 0.0033, Best Score: 0.0001, at: 17\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.0007232663920149207, 0.0037653339095413685, 0.01067816186696291), Mean of Top Loss: 0.0051, Best Score: 0.0001, at: 17\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.0004358962760306895, 0.0022871659602969885, 0.007591896690428257), Mean of Top Loss: 0.0034, Best Score: 0.0001, at: 17\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.00022565799008589238, 0.005391709040850401, 0.0064134118147194386), Mean of Top Loss: 0.004, Best Score: 0.0001, at: 17\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.00023988385510165244, 0.0006339189712889493, 0.005270423833280802), Mean of Top Loss: 0.002, Best Score: 0.0001, at: 17\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.00011535369412740692, 0.0042907604947686195, 0.0056852912530303), Mean of Top Loss: 0.0034, Best Score: 0.0001, at: 17\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0013243199791759253, 0.005332353990525007, 0.005865023471415043), Mean of Top Loss: 0.0042, Best Score: 0.0001, at: 17\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.0007040099008008838, 0.004424081649631262, 0.00448610307648778), Mean of Top Loss: 0.0032, Best Score: 0.0001, at: 17\n",
      "Generation: [31/10000], Top 3 agent rewards: (0.0001759442238835618, 0.004565071780234575, 0.008113449439406395), Mean of Top Loss: 0.0043, Best Score: 0.0001, at: 17\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.00014709577953908592, 0.0002660944883245975, 0.005592770874500275), Mean of Top Loss: 0.002, Best Score: 0.0001, at: 17\n",
      "Generation: [33/10000], Top 3 agent rewards: (4.923203232465312e-05, 0.00019705282466020435, 0.0032045338302850723), Mean of Top Loss: 0.0012, Best Score: 0.0001, at: 17\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.00013698710245080292, 0.00018007743346970528, 0.0020753222052007914), Mean of Top Loss: 0.0008, Best Score: 0.0, at: 33\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.00013585134001914412, 0.0003961794136557728, 0.0027860829140990973), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 33\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.0005312854191288352, 0.0015592037234455347, 0.004673569463193417), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 33\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.0001428567193215713, 0.0012523327022790909, 0.0019925073720514774), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 33\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.00012636360770557076, 0.0011179918656125665, 0.004106589127331972), Mean of Top Loss: 0.0018, Best Score: 0.0, at: 33\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.00010886711970670149, 0.0012565340148285031, 0.0032124530989676714), Mean of Top Loss: 0.0015, Best Score: 0.0, at: 33\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.00016304294695146382, 0.0003845331084448844, 0.006052312441170216), Mean of Top Loss: 0.0022, Best Score: 0.0, at: 33\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.00015736836940050125, 0.005026684142649174, 0.005231519695371389), Mean of Top Loss: 0.0035, Best Score: 0.0, at: 33\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.00015129198436625302, 0.005233231000602245, 0.007287770044058561), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 33\n",
      "Generation: [43/10000], Top 3 agent rewards: (9.676240006228909e-05, 0.004065954592078924, 0.007045126520097256), Mean of Top Loss: 0.0037, Best Score: 0.0, at: 33\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.00023099513782653958, 0.0060144695453345776, 0.008640452288091183), Mean of Top Loss: 0.005, Best Score: 0.0, at: 33\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.00022186498972587287, 0.004339663777500391, 0.007159125525504351), Mean of Top Loss: 0.0039, Best Score: 0.0, at: 33\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.0001371754624415189, 0.005940535105764866, 0.010307780466973782), Mean of Top Loss: 0.0055, Best Score: 0.0, at: 33\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.00035712201497517526, 0.0066762929782271385, 0.012358040548861027), Mean of Top Loss: 0.0065, Best Score: 0.0, at: 33\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.0006566959200426936, 0.0033779616933315992, 0.005234819371253252), Mean of Top Loss: 0.0031, Best Score: 0.0, at: 33\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.001206597895361483, 0.0017367883119732141, 0.009757244028151035), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [50/10000], Top 3 agent rewards: (0.00018956999701913446, 0.016802627593278885, 0.016901979222893715), Mean of Top Loss: 0.0113, Best Score: 0.0, at: 33\n",
      "Generation: [51/10000], Top 3 agent rewards: (0.00010011970152845606, 0.0026951574254781008, 0.00499120494350791), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 33\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.0012812729692086577, 0.0012988504022359848, 0.005330316256731749), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 33\n",
      "Generation: [53/10000], Top 3 agent rewards: (4.7294917749240994e-05, 0.000737236172426492, 0.0032560660038143396), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 33\n",
      "Generation: [54/10000], Top 3 agent rewards: (6.603282963624224e-05, 0.0016710901400074363, 0.0047241211868822575), Mean of Top Loss: 0.0022, Best Score: 0.0, at: 53\n",
      "Generation: [55/10000], Top 3 agent rewards: (0.0003836569667328149, 0.0034114609006792307, 0.004009249620139599), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 53\n",
      "Generation: [56/10000], Top 3 agent rewards: (9.954887354979292e-05, 0.006886802613735199, 0.007728508673608303), Mean of Top Loss: 0.0049, Best Score: 0.0, at: 53\n",
      "Generation: [57/10000], Top 3 agent rewards: (0.0003759553364943713, 0.014682912267744541, 0.017238741740584373), Mean of Top Loss: 0.0108, Best Score: 0.0, at: 53\n",
      "Generation: [58/10000], Top 3 agent rewards: (0.00030451564816758037, 0.0008324379450641572, 0.005651242099702358), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 53\n",
      "Generation: [59/10000], Top 3 agent rewards: (0.0002421633544145152, 0.003892316948622465, 0.008254478685557842), Mean of Top Loss: 0.0041, Best Score: 0.0, at: 53\n",
      "Generation: [60/10000], Top 3 agent rewards: (0.00038752914406359196, 0.0018434319645166397, 0.0075600468553602695), Mean of Top Loss: 0.0033, Best Score: 0.0, at: 53\n",
      "Generation: [61/10000], Top 3 agent rewards: (4.17650408053305e-05, 0.00398149061948061, 0.003998654894530773), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 53\n",
      "Generation: [62/10000], Top 3 agent rewards: (0.00010778500291053206, 0.0017487251898273826, 0.02070148102939129), Mean of Top Loss: 0.0075, Best Score: 0.0, at: 61\n",
      "Generation: [63/10000], Top 3 agent rewards: (0.0007328771171160042, 0.003772869473323226, 0.004263837356120348), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 61\n",
      "Generation: [64/10000], Top 3 agent rewards: (0.00027911303914152086, 0.0003525720094330609, 0.005234826821833849), Mean of Top Loss: 0.002, Best Score: 0.0, at: 61\n",
      "Generation: [65/10000], Top 3 agent rewards: (0.0009530208772048354, 0.007082995492964983, 0.008900242857635021), Mean of Top Loss: 0.0056, Best Score: 0.0, at: 61\n",
      "Generation: [66/10000], Top 3 agent rewards: (6.057766586309299e-05, 0.004831818398088217, 0.006681166589260101), Mean of Top Loss: 0.0039, Best Score: 0.0, at: 61\n",
      "Generation: [67/10000], Top 3 agent rewards: (0.00028078918694518507, 0.004497927147895098, 0.00540520902723074), Mean of Top Loss: 0.0034, Best Score: 0.0, at: 61\n",
      "Generation: [68/10000], Top 3 agent rewards: (0.0004237533430568874, 0.0025442163459956646, 0.005474749021232128), Mean of Top Loss: 0.0028, Best Score: 0.0, at: 61\n",
      "Generation: [69/10000], Top 3 agent rewards: (0.00036991972592659295, 0.004343737848103046, 0.005733704660087824), Mean of Top Loss: 0.0035, Best Score: 0.0, at: 61\n",
      "Generation: [70/10000], Top 3 agent rewards: (0.00021509516227524728, 0.00044686373439617455, 0.0039065564051270485), Mean of Top Loss: 0.0015, Best Score: 0.0, at: 61\n",
      "Generation: [71/10000], Top 3 agent rewards: (0.00040932156844064593, 0.0026878081262111664, 0.01732868328690529), Mean of Top Loss: 0.0068, Best Score: 0.0, at: 61\n",
      "Generation: [72/10000], Top 3 agent rewards: (0.00047152297338470817, 0.0076780724339187145, 0.008177277632057667), Mean of Top Loss: 0.0054, Best Score: 0.0, at: 61\n",
      "Generation: [73/10000], Top 3 agent rewards: (6.835655949544162e-05, 0.00042275586747564375, 0.004344121553003788), Mean of Top Loss: 0.0016, Best Score: 0.0, at: 61\n",
      "Generation: [74/10000], Top 3 agent rewards: (0.0001756058627506718, 0.0006586491363123059, 0.002459875075146556), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 61\n",
      "Generation: [75/10000], Top 3 agent rewards: (0.00018591746629681438, 0.002778543857857585, 0.0064672683365643024), Mean of Top Loss: 0.0031, Best Score: 0.0, at: 61\n",
      "Generation: [76/10000], Top 3 agent rewards: (8.41761429910548e-05, 0.006777034141123295, 0.013136380352079868), Mean of Top Loss: 0.0067, Best Score: 0.0, at: 61\n",
      "Generation: [77/10000], Top 3 agent rewards: (8.344893285539001e-05, 0.000796118110883981, 0.00532875582575798), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 61\n",
      "Generation: [78/10000], Top 3 agent rewards: (0.0002474737702868879, 0.0017756366869434714, 0.001972136553376913), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 61\n",
      "Generation: [79/10000], Top 3 agent rewards: (0.00012673392484430224, 0.002204241929575801, 0.008165223523974419), Mean of Top Loss: 0.0035, Best Score: 0.0, at: 61\n",
      "Generation: [80/10000], Top 3 agent rewards: (0.00021032270160503685, 0.0006842693546786904, 0.0011992325307801366), Mean of Top Loss: 0.0007, Best Score: 0.0, at: 61\n",
      "Generation: [81/10000], Top 3 agent rewards: (0.00018664934032130986, 0.00135155045427382, 0.002069237641990185), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 61\n",
      "Generation: [82/10000], Top 3 agent rewards: (8.70593503350392e-05, 0.002652957336977124, 0.002866246271878481), Mean of Top Loss: 0.0019, Best Score: 0.0, at: 61\n",
      "Generation: [83/10000], Top 3 agent rewards: (9.524032793706283e-05, 0.0018613969441503286, 0.0031458346638828516), Mean of Top Loss: 0.0017, Best Score: 0.0, at: 61\n",
      "Generation: [84/10000], Top 3 agent rewards: (0.00015797970991116017, 0.0027992345858365297, 0.01615864410996437), Mean of Top Loss: 0.0064, Best Score: 0.0, at: 61\n",
      "Generation: [85/10000], Top 3 agent rewards: (9.204383241012692e-05, 0.0022954612504690886, 0.014853440225124359), Mean of Top Loss: 0.0057, Best Score: 0.0, at: 61\n",
      "Generation: [86/10000], Top 3 agent rewards: (0.00012050003715557978, 0.0007557935896329582, 0.0026070144958794117), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 61\n",
      "Generation: [87/10000], Top 3 agent rewards: (0.0006870600627735257, 0.0024064534809440374, 0.005009208805859089), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 61\n",
      "Generation: [88/10000], Top 3 agent rewards: (0.000679348420817405, 0.003808767069131136, 0.004037293139845133), Mean of Top Loss: 0.0028, Best Score: 0.0, at: 61\n",
      "Generation: [89/10000], Top 3 agent rewards: (0.00016534188762307167, 0.0007484028465114534, 0.0021251761354506016), Mean of Top Loss: 0.001, Best Score: 0.0, at: 61\n",
      "Generation: [90/10000], Top 3 agent rewards: (0.0001993167825276032, 0.0016570929437875748, 0.002174871973693371), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 61\n",
      "Generation: [91/10000], Top 3 agent rewards: (0.00020045852579642087, 0.0007176106446422637, 0.00406095664948225), Mean of Top Loss: 0.0017, Best Score: 0.0, at: 61\n",
      "Generation: [92/10000], Top 3 agent rewards: (0.0001954097970155999, 0.0015181710477918386, 0.001934836502186954), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 61\n",
      "Generation: [93/10000], Top 3 agent rewards: (0.0003961794136557728, 0.0008000656962394714, 0.008501006290316582), Mean of Top Loss: 0.0032, Best Score: 0.0, at: 61\n",
      "Generation: [94/10000], Top 3 agent rewards: (0.00014850909064989537, 0.002049392322078347, 0.0025724503211677074), Mean of Top Loss: 0.0016, Best Score: 0.0, at: 61\n",
      "Generation: [95/10000], Top 3 agent rewards: (0.00044097224599681795, 0.0035479150246828794, 0.006034474354237318), Mean of Top Loss: 0.0033, Best Score: 0.0, at: 61\n",
      "Generation: [96/10000], Top 3 agent rewards: (0.00011298999743303284, 0.002775896806269884, 0.008163897320628166), Mean of Top Loss: 0.0037, Best Score: 0.0, at: 61\n",
      "Generation: [97/10000], Top 3 agent rewards: (0.0003267359279561788, 0.011885854415595531, 0.012086876668035984), Mean of Top Loss: 0.0081, Best Score: 0.0, at: 61\n",
      "Generation: [98/10000], Top 3 agent rewards: (0.0006398280966095626, 0.00464486051350832, 0.006605480331927538), Mean of Top Loss: 0.004, Best Score: 0.0, at: 61\n",
      "Generation: [99/10000], Top 3 agent rewards: (0.00015847259783186018, 0.004663601517677307, 0.009504628367722034), Mean of Top Loss: 0.0048, Best Score: 0.0, at: 61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [100/10000], Top 3 agent rewards: (0.0001225066080223769, 0.0015230218414217234, 0.005132337100803852), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 61\n",
      "Generation: [101/10000], Top 3 agent rewards: (0.00011854590411530808, 0.0024006948806345463, 0.008167966268956661), Mean of Top Loss: 0.0036, Best Score: 0.0, at: 61\n",
      "Generation: [102/10000], Top 3 agent rewards: (0.00015057944983709604, 0.0038948170840740204, 0.004365408327430487), Mean of Top Loss: 0.0028, Best Score: 0.0, at: 61\n",
      "Generation: [103/10000], Top 3 agent rewards: (0.00018684666429180652, 0.004570250399410725, 0.009942306205630302), Mean of Top Loss: 0.0049, Best Score: 0.0, at: 61\n",
      "Generation: [104/10000], Top 3 agent rewards: (0.0001760783197823912, 0.0021490640938282013, 0.005090420134365559), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 61\n",
      "Generation: [105/10000], Top 3 agent rewards: (0.0002587032213341445, 0.0013665978331118822, 0.004667394328862429), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 61\n",
      "Generation: [106/10000], Top 3 agent rewards: (0.0006026299670338631, 0.003797736018896103, 0.004290285054594278), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 61\n",
      "Generation: [107/10000], Top 3 agent rewards: (0.00022898016322869807, 0.002511359518393874, 0.002827746793627739), Mean of Top Loss: 0.0019, Best Score: 0.0, at: 61\n",
      "Generation: [108/10000], Top 3 agent rewards: (0.00030356532079167664, 0.0033687525428831577, 0.005378674250096083), Mean of Top Loss: 0.003, Best Score: 0.0, at: 61\n",
      "Generation: [109/10000], Top 3 agent rewards: (0.00017443245451431721, 0.0020559390541166067, 0.002953380811959505), Mean of Top Loss: 0.0017, Best Score: 0.0, at: 61\n",
      "Generation: [110/10000], Top 3 agent rewards: (0.00010786605707835406, 0.0075201173312962055, 0.008345849812030792), Mean of Top Loss: 0.0053, Best Score: 0.0, at: 61\n",
      "Generation: [111/10000], Top 3 agent rewards: (4.3531857954803854e-05, 0.0009557446464896202, 0.0010883498471230268), Mean of Top Loss: 0.0007, Best Score: 0.0, at: 61\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(False,train_data_no_tx,\"all\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"no_tx\",\"GA\",\"all\"), index=False)\n",
    "\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.0863703116774559, 0.13416780531406403, 0.16639010608196259), Mean of Top Loss: 0.129, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.0002071739436360076, 0.007818294689059258, 0.041231535375118256), Mean of Top Loss: 0.0164, Best Score: 0.0864, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.0002071739436360076, 0.011457917280495167, 0.030920153483748436), Mean of Top Loss: 0.0142, Best Score: 0.0002, at: 1\n",
      "Generation: [3/10000], Top 3 agent rewards: (0.03106374479830265, 0.032016489654779434, 0.04269375279545784), Mean of Top Loss: 0.0353, Best Score: 0.0002, at: 1\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.009622009471058846, 0.012691500596702099, 0.022526102140545845), Mean of Top Loss: 0.0149, Best Score: 0.0002, at: 1\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.0011793436715379357, 0.0071176327764987946, 0.01016108226031065), Mean of Top Loss: 0.0062, Best Score: 0.0002, at: 1\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.00037265088758431375, 0.0017549742478877306, 0.0020707419607788324), Mean of Top Loss: 0.0014, Best Score: 0.0002, at: 1\n",
      "Generation: [7/10000], Top 3 agent rewards: (0.0011793436715379357, 0.0016987613635137677, 0.004262946546077728), Mean of Top Loss: 0.0024, Best Score: 0.0002, at: 1\n",
      "Generation: [8/10000], Top 3 agent rewards: (0.00020849252177868038, 0.00041447809780947864, 0.0027599199675023556), Mean of Top Loss: 0.0011, Best Score: 0.0002, at: 1\n",
      "Generation: [9/10000], Top 3 agent rewards: (6.69179207761772e-05, 0.001214553602039814, 0.0051020304672420025), Mean of Top Loss: 0.0021, Best Score: 0.0002, at: 1\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0003742451954167336, 0.0007637474918738008, 0.0022632863838225603), Mean of Top Loss: 0.0011, Best Score: 0.0001, at: 9\n",
      "Generation: [11/10000], Top 3 agent rewards: (8.081520354608074e-05, 0.0001422734494553879, 0.0016226607840508223), Mean of Top Loss: 0.0006, Best Score: 0.0001, at: 9\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.00030560774030163884, 0.0011495546204969287, 0.0020038248039782047), Mean of Top Loss: 0.0012, Best Score: 0.0001, at: 9\n",
      "Generation: [13/10000], Top 3 agent rewards: (5.8262754464522004e-05, 0.0009949058294296265, 0.002517722314223647), Mean of Top Loss: 0.0012, Best Score: 0.0001, at: 9\n",
      "Generation: [14/10000], Top 3 agent rewards: (0.00028362878947518766, 0.0006918151630088687, 0.0012843788135796785), Mean of Top Loss: 0.0008, Best Score: 0.0001, at: 13\n",
      "Generation: [15/10000], Top 3 agent rewards: (5.8409383200341836e-05, 0.0002870047464966774, 0.0005355964531190693), Mean of Top Loss: 0.0003, Best Score: 0.0001, at: 13\n",
      "Generation: [16/10000], Top 3 agent rewards: (6.38138153590262e-05, 0.0007250666385516524, 0.0019302638247609138), Mean of Top Loss: 0.0009, Best Score: 0.0001, at: 13\n",
      "Generation: [17/10000], Top 3 agent rewards: (8.128659828798845e-05, 0.0010899693006649613, 0.0015261636581271887), Mean of Top Loss: 0.0009, Best Score: 0.0001, at: 13\n",
      "Generation: [18/10000], Top 3 agent rewards: (9.035206312546507e-05, 0.0023843098897486925, 0.004267956595867872), Mean of Top Loss: 0.0022, Best Score: 0.0001, at: 13\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.00015616380551364273, 0.0007127542048692703, 0.0033363080583512783), Mean of Top Loss: 0.0014, Best Score: 0.0001, at: 13\n",
      "Generation: [20/10000], Top 3 agent rewards: (9.321494871983305e-05, 0.006524193566292524, 0.007194350939244032), Mean of Top Loss: 0.0046, Best Score: 0.0001, at: 13\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.00016744472668506205, 0.0006595412269234657, 0.006942310370504856), Mean of Top Loss: 0.0026, Best Score: 0.0001, at: 13\n",
      "Generation: [22/10000], Top 3 agent rewards: (8.437054202659056e-05, 0.002960751298815012, 0.003783682594075799), Mean of Top Loss: 0.0023, Best Score: 0.0001, at: 13\n",
      "Generation: [23/10000], Top 3 agent rewards: (5.807807974633761e-05, 0.0020219634752720594, 0.004137862008064985), Mean of Top Loss: 0.0021, Best Score: 0.0001, at: 13\n",
      "Generation: [24/10000], Top 3 agent rewards: (5.812087329104543e-05, 0.0011145666940137744, 0.0019613897893577814), Mean of Top Loss: 0.001, Best Score: 0.0001, at: 23\n",
      "Generation: [25/10000], Top 3 agent rewards: (8.298881584778428e-05, 0.001536568976007402, 0.002083056140691042), Mean of Top Loss: 0.0012, Best Score: 0.0001, at: 23\n",
      "Generation: [26/10000], Top 3 agent rewards: (5.8908968640025705e-05, 0.0009460654109716415, 0.0016187077853828669), Mean of Top Loss: 0.0009, Best Score: 0.0001, at: 23\n",
      "Generation: [27/10000], Top 3 agent rewards: (6.110592221375555e-05, 0.0006962031475268304, 0.0022559133358299732), Mean of Top Loss: 0.001, Best Score: 0.0001, at: 23\n",
      "Generation: [28/10000], Top 3 agent rewards: (8.75324330991134e-05, 0.0014937197556719184, 0.00194769655354321), Mean of Top Loss: 0.0012, Best Score: 0.0001, at: 23\n",
      "Generation: [29/10000], Top 3 agent rewards: (5.984575909678824e-05, 0.0008716383599676192, 0.0014620177680626512), Mean of Top Loss: 0.0008, Best Score: 0.0001, at: 23\n",
      "Generation: [30/10000], Top 3 agent rewards: (7.710717909503728e-05, 0.0009494955302216113, 0.0019897150341421366), Mean of Top Loss: 0.001, Best Score: 0.0001, at: 23\n",
      "Generation: [31/10000], Top 3 agent rewards: (1.658562723605428e-05, 0.0009931583190336823, 0.001381887006573379), Mean of Top Loss: 0.0008, Best Score: 0.0001, at: 23\n",
      "Generation: [32/10000], Top 3 agent rewards: (8.113851799862459e-05, 0.001299033174291253, 0.0021488929633051157), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 31\n",
      "Generation: [33/10000], Top 3 agent rewards: (6.859311542939395e-05, 0.0009423604933544993, 0.0012221381766721606), Mean of Top Loss: 0.0007, Best Score: 0.0, at: 31\n",
      "Generation: [34/10000], Top 3 agent rewards: (5.188912837184034e-05, 0.0013415355933830142, 0.0027566205244511366), Mean of Top Loss: 0.0014, Best Score: 0.0, at: 31\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.00019323443120811135, 0.0020081044640392065, 0.004823177121579647), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 31\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.00017019947699736804, 0.0014379117637872696, 0.004500254523009062), Mean of Top Loss: 0.002, Best Score: 0.0, at: 31\n",
      "Generation: [37/10000], Top 3 agent rewards: (5.428279473562725e-05, 0.0028894084971398115, 0.003196138422936201), Mean of Top Loss: 0.002, Best Score: 0.0, at: 31\n",
      "Generation: [38/10000], Top 3 agent rewards: (6.351123010972515e-05, 0.0030697695910930634, 0.012028926983475685), Mean of Top Loss: 0.0051, Best Score: 0.0, at: 31\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.0014142299769446254, 0.005041901022195816, 0.006004498340189457), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 31\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.00011251273099333048, 0.0017194055253639817, 0.005415357183665037), Mean of Top Loss: 0.0024, Best Score: 0.0, at: 31\n",
      "Generation: [41/10000], Top 3 agent rewards: (7.532688323408365e-05, 0.000720485404599458, 0.002013178076595068), Mean of Top Loss: 0.0009, Best Score: 0.0, at: 31\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.00019492025603540242, 0.0014316352317109704, 0.0024698288179934025), Mean of Top Loss: 0.0014, Best Score: 0.0, at: 31\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.00015086322673596442, 0.0008172954549081624, 0.0021312928292900324), Mean of Top Loss: 0.001, Best Score: 0.0, at: 31\n",
      "Generation: [44/10000], Top 3 agent rewards: (6.924487388459966e-05, 0.00034300461993552744, 0.00035254444810561836), Mean of Top Loss: 0.0003, Best Score: 0.0, at: 31\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.0011356847826391459, 0.0012841039570048451, 0.0013962803641334176), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 31\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.0010097824269905686, 0.001747514121234417, 0.0021695448085665703), Mean of Top Loss: 0.0016, Best Score: 0.0, at: 31\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.00030094897374510765, 0.0023222079034894705, 0.003917346708476543), Mean of Top Loss: 0.0022, Best Score: 0.0, at: 31\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.0007104201358743012, 0.0013767831260338426, 0.005275648552924395), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 31\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.0007225466542877257, 0.0019420426106080413, 0.002143717836588621), Mean of Top Loss: 0.0016, Best Score: 0.0, at: 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [50/10000], Top 3 agent rewards: (0.0009531531832180917, 0.002714895410463214, 0.0031600543297827244), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 31\n",
      "Generation: [51/10000], Top 3 agent rewards: (0.0017476174980401993, 0.002537122229114175, 0.003393483115360141), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 31\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.0010583578841760755, 0.005075536202639341, 0.007612832821905613), Mean of Top Loss: 0.0046, Best Score: 0.0, at: 31\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.0037467346992343664, 0.004574372433125973, 0.009937791153788567), Mean of Top Loss: 0.0061, Best Score: 0.0, at: 31\n",
      "Generation: [54/10000], Top 3 agent rewards: (0.005964316427707672, 0.0063753752037882805, 0.006961763836443424), Mean of Top Loss: 0.0064, Best Score: 0.0, at: 31\n",
      "Generation: [55/10000], Top 3 agent rewards: (0.003680886933580041, 0.004201095551252365, 0.005127664655447006), Mean of Top Loss: 0.0043, Best Score: 0.0, at: 31\n",
      "Generation: [56/10000], Top 3 agent rewards: (0.0017255677375942469, 0.0020945516880601645, 0.003633221611380577), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 31\n",
      "Generation: [57/10000], Top 3 agent rewards: (0.00359813729301095, 0.0038535490166395903, 0.005282329861074686), Mean of Top Loss: 0.0042, Best Score: 0.0, at: 31\n",
      "Generation: [58/10000], Top 3 agent rewards: (0.008090609684586525, 0.009478813968598843, 0.010705232620239258), Mean of Top Loss: 0.0094, Best Score: 0.0, at: 31\n",
      "Generation: [59/10000], Top 3 agent rewards: (0.0012590092374011874, 0.003987073432654142, 0.0057670800015330315), Mean of Top Loss: 0.0037, Best Score: 0.0, at: 31\n",
      "Generation: [60/10000], Top 3 agent rewards: (0.008151032030582428, 0.008897864259779453, 0.009654834866523743), Mean of Top Loss: 0.0089, Best Score: 0.0, at: 31\n",
      "Generation: [61/10000], Top 3 agent rewards: (0.0006477307179011405, 0.010159661993384361, 0.011947400867938995), Mean of Top Loss: 0.0076, Best Score: 0.0, at: 31\n",
      "Generation: [62/10000], Top 3 agent rewards: (0.001788906753063202, 0.0070387511514127254, 0.015938205644488335), Mean of Top Loss: 0.0083, Best Score: 0.0, at: 31\n",
      "Generation: [63/10000], Top 3 agent rewards: (0.00096609111642465, 0.0029087394941598177, 0.01102457009255886), Mean of Top Loss: 0.005, Best Score: 0.0, at: 31\n",
      "Generation: [64/10000], Top 3 agent rewards: (0.005228493828326464, 0.007195294834673405, 0.00823983084410429), Mean of Top Loss: 0.0069, Best Score: 0.0, at: 31\n",
      "Generation: [65/10000], Top 3 agent rewards: (0.010166800580918789, 0.012341017834842205, 0.015717841684818268), Mean of Top Loss: 0.0127, Best Score: 0.0, at: 31\n",
      "Generation: [66/10000], Top 3 agent rewards: (0.0013001066399738193, 0.0013617590302601457, 0.0032241535373032093), Mean of Top Loss: 0.002, Best Score: 0.0, at: 31\n",
      "Generation: [67/10000], Top 3 agent rewards: (0.004043837543576956, 0.008521169424057007, 0.010987452231347561), Mean of Top Loss: 0.0079, Best Score: 0.0, at: 31\n",
      "Generation: [68/10000], Top 3 agent rewards: (0.008123332634568214, 0.031514089554548264, 0.04116680473089218), Mean of Top Loss: 0.0269, Best Score: 0.0, at: 31\n",
      "Generation: [69/10000], Top 3 agent rewards: (0.005824049469083548, 0.008509932085871696, 0.009451239369809628), Mean of Top Loss: 0.0079, Best Score: 0.0, at: 31\n",
      "Generation: [70/10000], Top 3 agent rewards: (0.005140062887221575, 0.009109909646213055, 0.010495288297533989), Mean of Top Loss: 0.0082, Best Score: 0.0, at: 31\n",
      "Generation: [71/10000], Top 3 agent rewards: (0.013578103855252266, 0.026387671008706093, 0.029928058385849), Mean of Top Loss: 0.0233, Best Score: 0.0, at: 31\n",
      "Generation: [72/10000], Top 3 agent rewards: (0.0013047336833551526, 0.001835695467889309, 0.014824733138084412), Mean of Top Loss: 0.006, Best Score: 0.0, at: 31\n",
      "Generation: [73/10000], Top 3 agent rewards: (0.014698086306452751, 0.015535018406808376, 0.018979981541633606), Mean of Top Loss: 0.0164, Best Score: 0.0, at: 31\n",
      "Generation: [74/10000], Top 3 agent rewards: (0.011686420999467373, 0.01261177659034729, 0.018211713060736656), Mean of Top Loss: 0.0142, Best Score: 0.0, at: 31\n",
      "Generation: [75/10000], Top 3 agent rewards: (0.01398514024913311, 0.026880847290158272, 0.027509039267897606), Mean of Top Loss: 0.0228, Best Score: 0.0, at: 31\n",
      "Generation: [76/10000], Top 3 agent rewards: (0.013585560023784637, 0.02309213951230049, 0.024378718808293343), Mean of Top Loss: 0.0204, Best Score: 0.0, at: 31\n",
      "Generation: [77/10000], Top 3 agent rewards: (0.004891972057521343, 0.012970567680895329, 0.033491116017103195), Mean of Top Loss: 0.0171, Best Score: 0.0, at: 31\n",
      "Generation: [78/10000], Top 3 agent rewards: (0.002530484227463603, 0.008771399967372417, 0.01174181792885065), Mean of Top Loss: 0.0077, Best Score: 0.0, at: 31\n",
      "Generation: [79/10000], Top 3 agent rewards: (0.01159173808991909, 0.013070343062281609, 0.035042859613895416), Mean of Top Loss: 0.0199, Best Score: 0.0, at: 31\n",
      "Generation: [80/10000], Top 3 agent rewards: (0.011768266558647156, 0.020492540672421455, 0.033797238022089005), Mean of Top Loss: 0.022, Best Score: 0.0, at: 31\n",
      "Generation: [81/10000], Top 3 agent rewards: (0.00844508595764637, 0.01888960786163807, 0.019368452951312065), Mean of Top Loss: 0.0156, Best Score: 0.0, at: 31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(False,train_data_no_tx,\"ec\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"no_tx\",\"GA\",\"ec\"), index=False)\n",
    "\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [0/10000], Top 3 agent rewards: (0.060644034296274185, 0.07548222690820694, 0.09878523647785187), Mean of Top Loss: 0.0783, Best Score: 99999, at: 0\n",
      "Generation: [1/10000], Top 3 agent rewards: (0.0015200189081951976, 0.006586889270693064, 0.00833222921937704), Mean of Top Loss: 0.0055, Best Score: 0.0606, at: 0\n",
      "Generation: [2/10000], Top 3 agent rewards: (0.004632276017218828, 0.022764593362808228, 0.023647582158446312), Mean of Top Loss: 0.017, Best Score: 0.0015, at: 1\n",
      "Generation: [3/10000], Top 3 agent rewards: (0.0031311288475990295, 0.006074161734431982, 0.011291725561022758), Mean of Top Loss: 0.0068, Best Score: 0.0015, at: 1\n",
      "Generation: [4/10000], Top 3 agent rewards: (0.0015146632213145494, 0.0065859099850058556, 0.0072944993153214455), Mean of Top Loss: 0.0051, Best Score: 0.0015, at: 1\n",
      "Generation: [5/10000], Top 3 agent rewards: (0.0015984714264050126, 0.005637045484036207, 0.006082799751311541), Mean of Top Loss: 0.0044, Best Score: 0.0015, at: 4\n",
      "Generation: [6/10000], Top 3 agent rewards: (0.0008141013095155358, 0.0015984714264050126, 0.00398523872718215), Mean of Top Loss: 0.0021, Best Score: 0.0015, at: 4\n",
      "Generation: [7/10000], Top 3 agent rewards: (4.688216358772479e-05, 0.0016725874738767743, 0.005908240098506212), Mean of Top Loss: 0.0025, Best Score: 0.0008, at: 6\n",
      "Generation: [8/10000], Top 3 agent rewards: (5.3439318435266614e-05, 0.0012608186807483435, 0.0021494333632290363), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 7\n",
      "Generation: [9/10000], Top 3 agent rewards: (3.293784902780317e-05, 0.00016953935846686363, 0.0059730554930865765), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 7\n",
      "Generation: [10/10000], Top 3 agent rewards: (0.0017889944138005376, 0.0034050464164465666, 0.003804506966844201), Mean of Top Loss: 0.003, Best Score: 0.0, at: 9\n",
      "Generation: [11/10000], Top 3 agent rewards: (6.929736264282838e-05, 0.0009956832509487867, 0.0014626426855102181), Mean of Top Loss: 0.0008, Best Score: 0.0, at: 9\n",
      "Generation: [12/10000], Top 3 agent rewards: (0.00041845906525850296, 0.0016274495283141732, 0.0016509437700733542), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 9\n",
      "Generation: [13/10000], Top 3 agent rewards: (0.00023585821327287704, 0.001309145474806428, 0.0016627205768600106), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 9\n",
      "Generation: [14/10000], Top 3 agent rewards: (8.090245682979003e-05, 0.002158352406695485, 0.0030809217132627964), Mean of Top Loss: 0.0018, Best Score: 0.0, at: 9\n",
      "Generation: [15/10000], Top 3 agent rewards: (8.090245682979003e-05, 0.0029921424575150013, 0.005071674007922411), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 9\n",
      "Generation: [16/10000], Top 3 agent rewards: (8.090245682979003e-05, 0.0022745358292013407, 0.005126885138452053), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 9\n",
      "Generation: [17/10000], Top 3 agent rewards: (2.4668037440278567e-05, 0.00025795697001740336, 0.0016500126803293824), Mean of Top Loss: 0.0006, Best Score: 0.0, at: 9\n",
      "Generation: [18/10000], Top 3 agent rewards: (0.0017173687228932977, 0.0022024756763130426, 0.00418889382854104), Mean of Top Loss: 0.0027, Best Score: 0.0, at: 17\n",
      "Generation: [19/10000], Top 3 agent rewards: (0.00017690520326141268, 0.0008901578257791698, 0.0024184456560760736), Mean of Top Loss: 0.0012, Best Score: 0.0, at: 17\n",
      "Generation: [20/10000], Top 3 agent rewards: (0.000377317366655916, 0.0007792990072630346, 0.0011490053730085492), Mean of Top Loss: 0.0008, Best Score: 0.0, at: 17\n",
      "Generation: [21/10000], Top 3 agent rewards: (0.00020720409520436078, 0.0008254590793512762, 0.0049019246362149715), Mean of Top Loss: 0.002, Best Score: 0.0, at: 17\n",
      "Generation: [22/10000], Top 3 agent rewards: (0.0007802751497365534, 0.001043838681653142, 0.0012528849765658379), Mean of Top Loss: 0.001, Best Score: 0.0, at: 17\n",
      "Generation: [23/10000], Top 3 agent rewards: (9.64895953075029e-05, 0.0017551452619954944, 0.0021317636128515005), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 17\n",
      "Generation: [24/10000], Top 3 agent rewards: (0.0005017314688302577, 0.0015505865449085832, 0.002144575584679842), Mean of Top Loss: 0.0014, Best Score: 0.0, at: 17\n",
      "Generation: [25/10000], Top 3 agent rewards: (0.0009076765854842961, 0.0011491397162899375, 0.0017280146712437272), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 17\n",
      "Generation: [26/10000], Top 3 agent rewards: (0.0022261240519583225, 0.003033558139577508, 0.0035453548189252615), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 17\n",
      "Generation: [27/10000], Top 3 agent rewards: (0.00020570566994138062, 0.0012066467897966504, 0.001903509721159935), Mean of Top Loss: 0.0011, Best Score: 0.0, at: 17\n",
      "Generation: [28/10000], Top 3 agent rewards: (0.0004328498034738004, 0.001373471925035119, 0.004504956770688295), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 17\n",
      "Generation: [29/10000], Top 3 agent rewards: (0.0008829635917209089, 0.0019750825595110655, 0.002209883416071534), Mean of Top Loss: 0.0017, Best Score: 0.0, at: 17\n",
      "Generation: [30/10000], Top 3 agent rewards: (0.0013078720076009631, 0.0025502152275294065, 0.003975747153162956), Mean of Top Loss: 0.0026, Best Score: 0.0, at: 17\n",
      "Generation: [31/10000], Top 3 agent rewards: (6.90992092131637e-05, 0.000873290526214987, 0.003846853505820036), Mean of Top Loss: 0.0016, Best Score: 0.0, at: 17\n",
      "Generation: [32/10000], Top 3 agent rewards: (0.00035839900374412537, 0.0009909379296004772, 0.005783701781183481), Mean of Top Loss: 0.0024, Best Score: 0.0, at: 17\n",
      "Generation: [33/10000], Top 3 agent rewards: (0.000656165590044111, 0.0038174849469214678, 0.004207168705761433), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 17\n",
      "Generation: [34/10000], Top 3 agent rewards: (0.0001295918773394078, 0.0006546378135681152, 0.0016942479414865375), Mean of Top Loss: 0.0008, Best Score: 0.0, at: 17\n",
      "Generation: [35/10000], Top 3 agent rewards: (0.00026095841894857585, 0.00034982390934601426, 0.024138255044817924), Mean of Top Loss: 0.0082, Best Score: 0.0, at: 17\n",
      "Generation: [36/10000], Top 3 agent rewards: (0.0012943957699462771, 0.004278262611478567, 0.014983736909925938), Mean of Top Loss: 0.0069, Best Score: 0.0, at: 17\n",
      "Generation: [37/10000], Top 3 agent rewards: (0.0010332532692700624, 0.0010392408585175872, 0.01102126482874155), Mean of Top Loss: 0.0044, Best Score: 0.0, at: 17\n",
      "Generation: [38/10000], Top 3 agent rewards: (0.0007237737881951034, 0.004531475715339184, 0.005818426143378019), Mean of Top Loss: 0.0037, Best Score: 0.0, at: 17\n",
      "Generation: [39/10000], Top 3 agent rewards: (0.0008845487609505653, 0.0012320906389504671, 0.004840030334889889), Mean of Top Loss: 0.0023, Best Score: 0.0, at: 17\n",
      "Generation: [40/10000], Top 3 agent rewards: (0.00031667150324210525, 0.002273947698995471, 0.005026774946600199), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 17\n",
      "Generation: [41/10000], Top 3 agent rewards: (0.00085133605170995, 0.0014639628352597356, 0.004100142512470484), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 17\n",
      "Generation: [42/10000], Top 3 agent rewards: (0.0012081234017387033, 0.007609088905155659, 0.008363236673176289), Mean of Top Loss: 0.0057, Best Score: 0.0, at: 17\n",
      "Generation: [43/10000], Top 3 agent rewards: (0.0013350257650017738, 0.009236255660653114, 0.011442909948527813), Mean of Top Loss: 0.0073, Best Score: 0.0, at: 17\n",
      "Generation: [44/10000], Top 3 agent rewards: (0.0013341634767130017, 0.0023207657504826784, 0.005080053117126226), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 17\n",
      "Generation: [45/10000], Top 3 agent rewards: (0.0013341634767130017, 0.004389907233417034, 0.004660256206989288), Mean of Top Loss: 0.0035, Best Score: 0.0, at: 17\n",
      "Generation: [46/10000], Top 3 agent rewards: (0.0009315437637269497, 0.0038334899581968784, 0.00688139395788312), Mean of Top Loss: 0.0039, Best Score: 0.0, at: 17\n",
      "Generation: [47/10000], Top 3 agent rewards: (0.001404013717547059, 0.008349054493010044, 0.012845595367252827), Mean of Top Loss: 0.0075, Best Score: 0.0, at: 17\n",
      "Generation: [48/10000], Top 3 agent rewards: (0.0012642090441659093, 0.003784617641940713, 0.005751853343099356), Mean of Top Loss: 0.0036, Best Score: 0.0, at: 17\n",
      "Generation: [49/10000], Top 3 agent rewards: (0.0013350257650017738, 0.004245414398610592, 0.012970917858183384), Mean of Top Loss: 0.0062, Best Score: 0.0, at: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: [50/10000], Top 3 agent rewards: (0.0013341634767130017, 0.0019989514257758856, 0.013264670968055725), Mean of Top Loss: 0.0055, Best Score: 0.0, at: 17\n",
      "Generation: [51/10000], Top 3 agent rewards: (0.0013350257650017738, 0.0028127965051680803, 0.00803267490118742), Mean of Top Loss: 0.0041, Best Score: 0.0, at: 17\n",
      "Generation: [52/10000], Top 3 agent rewards: (0.0013350257650017738, 0.009218530729413033, 0.027562513947486877), Mean of Top Loss: 0.0127, Best Score: 0.0, at: 17\n",
      "Generation: [53/10000], Top 3 agent rewards: (0.0013341634767130017, 0.025502508506178856, 0.02692510187625885), Mean of Top Loss: 0.0179, Best Score: 0.0, at: 17\n",
      "Generation: [54/10000], Top 3 agent rewards: (0.0013341634767130017, 0.01474512554705143, 0.015317254699766636), Mean of Top Loss: 0.0105, Best Score: 0.0, at: 17\n",
      "Generation: [55/10000], Top 3 agent rewards: (0.0013341634767130017, 0.002105176215991378, 0.018555330112576485), Mean of Top Loss: 0.0073, Best Score: 0.0, at: 17\n",
      "Generation: [56/10000], Top 3 agent rewards: (0.0013341634767130017, 0.001498207449913025, 0.019663263112306595), Mean of Top Loss: 0.0075, Best Score: 0.0, at: 17\n",
      "Generation: [57/10000], Top 3 agent rewards: (0.0010177158983424306, 0.0013341634767130017, 0.006292248610407114), Mean of Top Loss: 0.0029, Best Score: 0.0, at: 17\n",
      "Generation: [58/10000], Top 3 agent rewards: (0.0016919765621423721, 0.0017667984357103705, 0.004091411828994751), Mean of Top Loss: 0.0025, Best Score: 0.0, at: 17\n",
      "Generation: [59/10000], Top 3 agent rewards: (0.0006533927517011762, 0.0013350257650017738, 0.0019578863866627216), Mean of Top Loss: 0.0013, Best Score: 0.0, at: 17\n",
      "Generation: [60/10000], Top 3 agent rewards: (0.0007773853722028434, 0.00810211431235075, 0.008310338482260704), Mean of Top Loss: 0.0057, Best Score: 0.0, at: 17\n",
      "Generation: [61/10000], Top 3 agent rewards: (0.0013350257650017738, 0.012819267809391022, 0.015437603928148746), Mean of Top Loss: 0.0099, Best Score: 0.0, at: 17\n",
      "Generation: [62/10000], Top 3 agent rewards: (0.0013350257650017738, 0.012128311209380627, 0.020989881828427315), Mean of Top Loss: 0.0115, Best Score: 0.0, at: 17\n",
      "Generation: [63/10000], Top 3 agent rewards: (0.0013350257650017738, 0.002027407754212618, 0.01799219846725464), Mean of Top Loss: 0.0071, Best Score: 0.0, at: 17\n",
      "Generation: [64/10000], Top 3 agent rewards: (0.0013350257650017738, 0.0021295915357768536, 0.005473610479384661), Mean of Top Loss: 0.003, Best Score: 0.0, at: 17\n",
      "Generation: [65/10000], Top 3 agent rewards: (0.0013350257650017738, 0.0013830404495820403, 0.0032343852799385786), Mean of Top Loss: 0.002, Best Score: 0.0, at: 17\n",
      "Generation: [66/10000], Top 3 agent rewards: (0.0013350257650017738, 0.004744257312268019, 0.005568935070186853), Mean of Top Loss: 0.0039, Best Score: 0.0, at: 17\n",
      "Generation: [67/10000], Top 3 agent rewards: (0.0013350257650017738, 0.0016831925604492426, 0.0031954178120940924), Mean of Top Loss: 0.0021, Best Score: 0.0, at: 17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = GA_Iteration(False,train_data_no_tx,\"ms\")\n",
    "loss = list(zip(*loss_history))[1]\n",
    "plt.figure(1,figsize=(16,7))\n",
    "plt.scatter(range(len(loss)),loss)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(loss_history).to_csv('Models/loss_history_{}_{}_{}.csv'.format(\"no_tx\",\"GA\",\"ms\"), index=False)\n",
    "\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
